the generalization problem is posed as follows : for a given concept class c , an unknown target t , an arbitrary error rate e , and confidence 6 , how many examples do we have to draw and classify from an arbitrary distribution
at this point , we need to draw attention to the distinction between a neural network 's architecture and its configuration .2 the architecture of a neural network refers to those parameters of the network that do not change during training ; in our case , these will be the network 's topology and transfer functions .
having trained on training set 5 ' , we can say that the network configuration implements a concept c that is consistent with training set 5m we will use c to denote both the concept c and the network .6 that implements it .
this creates a background bias over the domain that is weighted by the input distribution 19 : the networks that have the least error on these background patterns will be the ones that are the most specific according to v. in order to allow the network to converge on the actual training examples in spite of the background examples , we must balance the influence of background examples against that of the training data .
