many natural language processing applications , such as question answering -LRB- qa -RRB- , information extraction -LRB- ie -RRB- , -LRB- multi-document -RRB- summarization , and machine translation -LRB- mt -RRB- evaluation , need a model for this variability phenomenon in order to recognize that a particular target meaning can be inferred from different text variants .
as in other evaluation tasks our definition of textual entailment is operational , and corresponds to the judgment criteria given to the annotators who decide whether this relationship holds between a given pair of texts or not .
within each application setting the annotators selected both positive entailment examples -LRB- true -RRB- , where t is judged to entail h , as well as negative examples -LRB- false -RRB- , where entailment does not hold -LRB- a 50 % -50 % split -RRB- .
the remaining examples were considered as the gold standard for evaluation , split to 567 examples in the development set and 800 in the test set , and evenly split to true \/ false examples .
this is slightly different from the common use of average precision measures in ir and qa , in which systems rank the results by confidence of positive
a system which does well in recognizing when entailment does not hold would do just as well in terms of accuracy and cws as a system tailored to recognize true examples .
