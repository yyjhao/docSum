for example , when learning a model for penn treebank-style part-of-speech tagging in english , we may list the 45 target tags and a few examples of each tag -LRB- see figure 4 for a concrete prototype list for this task -RRB- .
the overall approach of grenager et al. -LRB- 2005 -RRB- typifies the process involved in fully unsupervised learning on new domain : they first alter the structure of their hmm so that diagonal transitions are preferred , then modify the transition structure to explicitly model boundary tokens , and so
we automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often .
each prototype word is also its own prototype -LRB- since a word has maximum similarity to itself -RRB- , so when we lock the prototype to a label , we are also pushing all the words distributionally similar to that prototype towards that label .
an important characteristic of this domain -LRB- see figure 1 -LRB- a -RRB- -RRB- is that the hidden labels tend to be sticky , in that fields tend to consist of runs of the same label , as in figure 1 -LRB- c -RRB- , in contrast with part-of-speech tagging , where we rarely see adjacent tokens given the same label .
