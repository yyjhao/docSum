large tables lead to large data structures that require more resources and more time to process and , more importantly , effort directed in handling large tables could likely be more usefully employed in
phrase translation model probabilities are features of the form : the forward phrase probabilities p -LRB- ~ tj ~ s -RRB- are not used as features , but only as a filter on the set of possible translations : for each source phrase s ~ that matches some ngram in s , only the 30 top-ranked translations t ~ according to p -LRB- ~ tj ~ s -RRB- are retained .
the idea behind significance pruning of phrasetables is that not all of the phrase pairs in a phrasetable are equally supported by the data and that many of the weakly supported pairs could be removed because : the chance of them occurring again might be low , and their occurrence in the given corpus may be the result of an artifact -LRB- a combination of effects where several estimates artificially compensate for one another -RRB- .
a special series of runs was done for threshold 14 with all of these singletons removed to see whether at these thresholds it was the significance level or the pruning of phrase pairs with -LRB- c -LRB- ~ s , ~ t -RRB- = 1 -RRB- that was more important .
