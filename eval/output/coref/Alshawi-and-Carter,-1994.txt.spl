we do
as mentioned earlier , the preference score is a weighted sum of a set of preference functions : each preference function fb takes a complete qlf representation qi as input , returning a numerical score sib , the overall preference score being computed by summing over the product of function scores with their associated scaling factors cb .
this least-squares set of scaling factors achieves quite good disambiguation performance -LRB- see section 4 below -RRB- but is not truly optimal because of the inherent nonlinearity of the goal , which is to maximize the proportion of sentences for which a correct qlf is selected , rather than to approximate training scores -LRB- even relativized ones .
from these five functions on triples we define five semantic collocation preference functions applied to qlfs , in each case by averaging over the result of applying the function to each triple derived from a qlf .
however , the difference between the chi and chi-squared functions is no longer quite so clear cut , and the relative advantage of the mean distance function compared with the chi function is less , perhaps because other preference functions make up for some shortfall of the chi function that is , at least in part , taken account of by the mean distance function .
