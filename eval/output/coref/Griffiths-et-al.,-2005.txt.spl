each component divides words into finer groups according to a different criterion : the function words are divided into syntactic classes , and the content words are divided into semantic topics .
hmms and probabilistic context-free grammars -LRB- e.g. , -LSB- 5 -RSB- -RRB- generate documents purely based on syntactic relations among unobserved word classes , while bag-of-words models like naive bayes or topic models -LRB- e.g. , -LSB- 6 -RSB- -RRB- generate documents based on semantic correlations between words , independent of word order .
the model is defined in terms of three sets of variables : a sequence of words w = -LCB- wl , ... , wn -RCB- , with each wi being one of w words , a sequence of topic assignments z = -LCB- zl , ... zn -RCB- , with each zi being one of t topics , and a sequence of classes c = -LCB- c1 , ... , cn -RCB- , with each ci being one of c classes .
in addition to running the composite model with t = 200 and c = 20 , we examined two special cases : t = 200 , c = 2 , being a model where the only hmm classes are the start \/ end and semantic classes , and thus equivalent to latent dirichlet allocation -LRB- lda ; -LSB- 6 -RSB- -RRB- ; and t = 1 , c = 20 , being an hmm in which the semantic class distribution does not vary across documents , and simply has a different hyperparameter from the other classes .
the
