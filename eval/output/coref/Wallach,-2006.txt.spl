such models typically fall into one of two categoriesthose that generate each word on the basis of some number of preceding words or word classes and those that generate words based on latent topic variables inferred from word correlations independent of the order in which the words appear .
in addition to exhibiting better predictive performance than either mackay and petos language model or latent dirichlet allocation , the topics inferred using the new model are typically less dominated by function words than are topics inferred from the same corpora using latent dirichlet allocation .
in my implementation , each fixed-point iteration takes time that is proportional to s and -LRB- at worst -RRB- n. for latent dirichlet allocation and the new model with prior 1 , the time taken to perform the m-step is therefore at worst proportional to s , n and the number of iterations taken to reach convergence .
in latent dirichlet allocation , the latent topic for a given word is inferred using the identity of the word , the number of times the word has previously been assumed to be generated by each topic , and the number of times each topic has been used in the current document .
firstly , the predictive accuracy of the new model , especially when
