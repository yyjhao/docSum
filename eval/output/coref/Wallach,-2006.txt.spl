such models typically fall into one of two categoriesthose that generate each word on the basis of some number of preceding words or word classes and those that generate words based on latent topic variables inferred from word correlations independent of the order in which the words appear .
in addition to exhibiting better predictive performance than either mackay and petos language model or latent dirichlet allocation , the topics inferred using the new model are typically less dominated by function words than are topics inferred from the same corpora using latent dirichlet allocation .
the priors over 4 -RRB- used in both mackay and petos language model and blei et al. s latent dirichlet allocation are coupled priors : learning the probability vector for a single context , oj the case of mackay and petos model and ok in blei et al. s , gives information about the probability vectors in other contexts , j ' and k ' respectively .
in my
in latent dirichlet allocation , the latent topic for a given word is inferred using the identity of the word , the number of times the word has previously been assumed to be generated by each topic , and the number of times each topic has been used in the current document .
