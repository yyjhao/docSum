in this case exceedingly large amounts of training data is needed and as well as a very large number of topics k. even in this case there is a danger that the model will be overflown by very fine-grain global topics or the resulting topics will be intersection of global topics and ratable aspects , like location for hotels in new york .
the mg-lda model infers only types
top words for the discovered local topics and for some of the global topics of mg-lda models are presented in table 2 - table 3 , one topic per line , along with selected topics from the lda models .7 we manually assigned labels to coherent topics to reflect our interpretation of their meaning .
for the hotels reviews we present results of the mg-lda model with 15 local topics and 45 global topics and results of the lda model with 45 topics .
their topic-sentiment model -LRB- tsm -RRB- is essentially equivalent to the plsa aspect model with two additional topics .11 one of these topics has a prior towards positive sentiment words and the other towards negative sentiment words , where both priors are induced from sentiment labeled data .
we propose a model called multi-grain lda -LRB- mg-lda -RRB- , which models two distinct types of topics : global topics and local topics .
