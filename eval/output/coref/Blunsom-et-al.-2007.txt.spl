the distribution is globally normalised by the partition function , za -LRB- f -RRB- , which sums out the numerator in -LRB- 1 -RRB- for every derivation -LRB- and therefore every translation -RRB- of f : given -LRB- 1 -RRB- , the conditional probability of a target translation given the source is the sum over all of its derivations : most prior work in smt , both generative and discriminative , has approximated the sum over derivations by choosing a single best derivation using a viterbi or beam search algorithm .
our model evaluation was motivated by the following questions : -LRB- 1 -RRB- the effect of maximizing translations rather than derivations in training and decoding ; -LRB- 2 -RRB- whether a regularised model performs better than a maximum likelihood model ; -LRB- 3 -RRB- how the performance of our model compares with a frequency count based hierarchical system ; and -LRB- 4 -RRB- how translation performance scales with the number of training examples .
derivational ambiguity table
the feature set includes : a trigram language model -LRB- lm -RRB- trained on the english side of the unfiltered europarl corpus ; direct and reverse translation scores estimated as relative frequencies -LRB- pd , pr -RRB- ; lexical translation scores -LRB- pd y , prey -RRB- , a binary flag for the glue rule which allows the model to -LRB- dis -RRB- favor monotone translation -LRB- gr -RRB- ; and rule and target word counts -LRB- rc , wc -RRB- .
