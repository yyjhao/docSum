the aligning process is just to find the alignment segmentation ^ between the two strings that maximizes the joint probability : a set of transliteration pairs that is derived from the aligning process forms a transliteration table , which is in turn used in the transliteration decoding .
we use a database from the bilingual dictionary chinese transliteration of foreign personal names which was edited by
for a test set w composed of v names , where each name has been aligned into a sequence of transliteration pair tokens , we can calculate the probability of test set number of aligned transliteration pair tokens in the data w. the perplexity ppp -LRB- w -RRB- of a model is the reciprocal of the average probability assigned by the model to each aligned pair in the test set w as ppp -LRB- w -RRB- = 2 h -LRB- w -RRB- .
n-gram tm seems to have better captured the dynamics of transliteration units ; the backoff smoothing of n-gram tm is more effective than that of id3 ; unlike n-gram tm , id3 requires a separate aligning process for bilingual dictionary .
the first 4 setups by virga et al all adopted the phoneme-based approach in the following steps : english name to english phonemes ; english phonemes to chinese pinyin ; 3 -RRB- chinese pinyin to chinese characters .
