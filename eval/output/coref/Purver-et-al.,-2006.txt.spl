we use gibbs sampling , drawing the topic assignment for each word , zu , z , conditioned on all other topic assignments , z -LRB- u , z -RRB- , all topic change indicators , c , and all words , w ; and then drawing the topic change indicator for each utterance , cu , conditioned
following previous work on probabilistic topic models -LRB- hofmann , 1999 ; blei et al. , 2003 ; griffiths and steyvers , 2004 -RRB- , we model each utterance as being generated from a particular distribution over topics , where each topic is a probability distribution over words .
of these lists , 40 contained the most indicative words for each of the 10 topics from different models : the topic segmentation model ; a topic model that had the same number of segments but with fixed evenly spread segmentation boundaries ; an equivalent with randomly placed segmentation boundaries ; and the hmm .
however , while hmm approaches allow a segmentation of the data by topic , they do not allow adaptively combining different topics into segments : while a new segment can be modelled as being identical to a topic that has already been observed , it can not be modelled as a combination of the previously observed topics .1 note that while -LRB- imai et al. , 1997 -RRB- s hmm approach allows topic mixtures , it requires supervision with hand-labelled topics .
