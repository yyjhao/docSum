the generative process in our model may be viewed as an extension of the dirichlet process mixture model : the group labels in our model influence the choice of entities for each author reference in a paper .
the first term is the probability of group t in document di , the second is the probability of author a in group t and the third is the probability of
the sampling algorithm now also samples the author attributes vi iteratively , conditioned on the references and current author assignments , along with sampling the group and entity labels for each reference .
the different ways of distorting or modifying an author attribute to an author reference in a paper is captured by the noise model n. the noise model handles first , middle and last names independently .
basic inference with gibbs sampling we first describe a novel but simple gibbs sampling algorithm for iteratively sampling the values of the hidden group and entity labels for each reference conditioned on the existing labels of all other references .
similarly , when 0 = 0 , a reference has to pick a group label from other references to the same author , and also an author label from other references with the same group label .
