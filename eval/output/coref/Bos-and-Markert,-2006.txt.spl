we perform model building using two different model builders : we use paradox to find the size of the domain , and then use mace to construct a minimal model giving that domain size .
else , if there is no proof for 3 or 4 , but there is a proof for 2 , then again it is very likely that t entails h. finally , if the model size difference between the models generated for 5 and 6 is very small then it is likely that t entails h. background knowledge .
we generate background knowledge -LRB- bk -RRB- using two kinds of sources : hyponymy relations from wordnet , and a set of manually coded inference rules expressing general knowledge -LRB- see appendix for examples -RRB- .
put differently , the domain size for t + h would equal the domain size of t. in contrast , if t does not entail h , h normally introduce some new information -LRB- except when it contains negated information -RRB- , and this will be reflected in the domain size of t + h , which then is larger than the domain size of t. it turns out that this difference between the domain sizes is a useful way of measuring the likelihood of entailment .
for a combination of deep and shallow features we took into account
