we computed the document frequency for each unique term in the training corpus and removed from the feature space those terms whose document frequency was less than some predetermined threshold .
we computed for each category the x2 statistic between each unique term in a training corpus and that category , and then combined the category - specific scores of each term into two scores : the computation of chi scores has a quadratic complexity , similar to mi and ig .
in a recent evaluation of classification methods -LSB- 26 -RSB- on the reuters newswire collection -LRB- next section -RRB- , the break-even point values were 85 % for both knn and llsf , outperforming all the other systems evaluated on the same collection , including symbolic rule learning by ripper -LRB- 80 % -RRB- -LSB- 3 -RSB- , swap - 1 -LRB- 79 % -RRB- -LSB- 1 -RSB- and charade -LRB- 78 % -RRB- -LSB- 16 -RSB- , a decision approach using
4.3 primary results figure 1 displays the performance curves for knn on reuters -LRB- 9,610 training documents , and 3,662 test documents -RRB- after term selection using ig , df , ts , mi and chi thresholding , respectively .
this is an evaluation of feature selection methods in dimensionality reduction for text categorization at all the reduction levels of aggressiveness , from using the full vocabulary -LRB- except stop words -RRB- as the feature space , to removing 98 % of the unique terms .
