numerous studies show that effective text classifiers can be produced by supervised learning methods , including support vector machines -LRB- svms -RRB- -LSB- 11 , 14 , 33 -RSB- , regularized logistic regression -LSB- 9 , 33 -RSB- , and
several recent papers have modified learning approaches naive bayes -LSB- 13 , 16 -RSB- , logistic regression -LRB- fit with a boosting - style algorithm -RRB- -LSB- 25 -RSB- , and svms -LSB- 31 -RSB- to use domain knowledge in text categorization .
chelba and acero -LSB- 5 -RSB- use out-of-task labeled examples in logistic regression training of a text capitalizer , and use the resulting map estimate as the mode vector of a bayesian prior for training with in-task examples .
some papers explore several values for the prior variance -LSB- 15 , 33 -RSB- , others use a single value but do not say how it was chosen -LSB- 19 , 32 -RSB- , and others choose the variance by cross-validation on the training set -LSB- 9 -RSB- .
table 2 summarizes the types of domain knowledge used , and the number of domain knowledge texts used to compute significance values for the var \/ tfidf and mode \/ tfidf methods .
the resulting training sets had 2 to 139 positive examples for categories in the bio articles collection , 9 to 184 positive examples for categories in the modapte top 10 collection , and 0 to 22 positive examples for categories in the rcv1 a-b regions collection .
