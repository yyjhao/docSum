for a given wikipedia entity e g e , let e. 0 be the set of categories to which e belongs -LRB- i.e. e s immediate categories and all their ancestors in the wikipedia taxonomy -RRB- .
the second step constructs the actual dictionary d as follows : the set of entries in d consists of all strings that may denote a named entity , i.e. if e e e is a named entity , then its title name e.title , its redirect names e.r , and its disambiguation names e.d are all added as entries in d. each entry string d e d is mapped to d .
the feature vector -LRB- d -LRB- q , ek -RRB- contains a dedicated feature 0eos for cosine similarity , and i v i x i c features 0w , e corresponding to combinations of words w from the wikipedia vocabulary v and categories c from the wikipedia taxonomy c : the weight vector w models the magnitude of each word-category correlation , and can be learned by training on the query dataset described at the beginning of section 4 .
while creating a separate model for each named entity is not feasible there are 94,875 titles under people by occupation named entity disambiguation can nevertheless benefit from correlations between wikipedia cate gories and features traditionally used in wsd such
