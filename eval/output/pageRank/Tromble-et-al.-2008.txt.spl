kumar and byrne -LRB- 2004 -RRB- show that mbr decoding gives optimal performance when the loss function is matched to the evaluation criterion ; in particular , mbr under the sentence-level bleu loss function -LRB- papineni et al. , 2001 -RRB- gives gains on bleu .
given such a loss function l -LRB- e , e ' -RRB- between an automatic translation e ' and the reference e , and an underlying probability model p -LRB- eif -RRB- , the mbr decoder has the following form -LRB- goel and byrne , 2000 ; kumar and byrne , 2004 -RRB- : we are interested in performing mbr decoding under a sentence-level bleu score -LRB- papineni et al. , 2001 -RRB- which behaves like a gain function : it varies between 0 and 1 , and a larger value reflects a higher similarity .
these gains are obtained on top of a baseline system that has competitive performance relative to the results reported in the nist 2008 evaluation .7 this demonstrates the effectiveness of lattice mbr decoding as a realization of mbr decoding which yields substantial gains over the n-best implementation .
the results show that when restricted to the 1000 - best list , lattice mbr performs slightly better than n-best mbr -LRB- with sentence bleu -RRB- on aren \/ enzh while n-best mbr is better on zhen .
lattice mbr gives consistent improvements in translation performance over n-best mbr decoding , which is used in many
