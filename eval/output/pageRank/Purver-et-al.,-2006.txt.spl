we use gibbs sampling , drawing the topic assignment for each word , zu , z , conditioned on all other topic assignments , z -LRB- u , z -RRB- , all topic change indicators , c , and all words , w ; and then drawing the topic change indicator for each utterance , cu , conditioned on all other topic change indicators , cu , all topic assignments z , and all words w .
however , while hmm approaches allow a segmentation of the data by topic , they do not allow adaptively combining different topics into segments : while a new segment can be
of these lists , 40 contained the most indicative words for each of the 10 topics from different models : the topic segmentation model ; a topic model that had the same number of segments but with fixed evenly spread segmentation boundaries ; an equivalent with randomly placed segmentation boundaries ; and the hmm .
interestingly , using an even distribution of boundaries but allowing the topic model to infer topics performs similarly well with even segmentation , but badly with random segmentation topic quality is thus not very susceptible to the precise segmentation of the text , but does require some reasonable approximation -LRB- on icsi data , an even segmentation gives a pk of about 50 % , while random segmentations can do much worse -RRB- .
