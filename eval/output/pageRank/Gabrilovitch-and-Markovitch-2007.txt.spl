we use machine learning techniques to build a semantic interpreter that maps fragments of natural language text into a weighted sequence of wikipedia concepts ordered by their relevance to the input .
entries of this vector reflect the relevance of the corresponding concepts to text t. to compute semantic relatedness of a pair of text fragments we compare their vectors using the cosine metric .
since text categorization is a supervised learning task , words occurring in the training documents serve as valuable features ; consequently , in that work we used wikipedia concepts to augment the bag of words .
we processed the text of these articles by removing stop words and rare words , and stemming the remaining words ; this yielded 389,202 distinct terms , which served for representing wikipedia concepts as attribute vectors .
prior work in the field pursued three main directions : comparing text fragments as bags of words in vector space -LSB- baeza-yates and ribeiro-neto , 1999 -RSB- , using lexical
in this paper we deal with semantic relatedness rather than semantic similarity or semantic distance , which are also often used in the literature .
on the other hand , our approach represents each word as a weighted vector of wikipedia concepts , and semantic relatedness is then computed by comparing the two concept vectors .
