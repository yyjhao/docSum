sch olkopf -LSB- 14 -RSB- showed that the prior knowledge can be incorporated with the appropriate kernel function , and fung -LSB- 4 -RSB- showed prior knowledge in the form of multiple polyhedral sets can be used with a reformulation of svm .
the proposed weighted margin support vector machine -LRB- wmsvm -RRB- can generalize on imperfectly labeled training dataset because each pattern in the dataset associates not only with a category label but also a confidence value that varies
given the true training dataset and pseudo training dataset , we now have two possibly conflicting goals in minimizing the empirical risk when constructing a predictor : -LRB- 1 -RRB- fit the true training dataset , and -LRB- 2 -RRB- fit the pseudo training dataset and thus fit the prior knowledge .
let the first m training examples be the labeled examples , and the rest be the pseudo examples , the objective function of primal problem is given : here the functionality of the parameter c is the same as the standard svm to control the balance between the model complexity and training error .
the performance of wmsvm with the prior knowledge on reuters is comparable to that of transductive svm -LSB- 8 -RSB- , but the training time is much less as only one iteration of svm training is needed .
