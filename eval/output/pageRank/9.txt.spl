finally , the algorithm creates similarity edges between each pair of nodes that has similarity scores lower than 7 , but higher than a low-confidence threshold e. this means this pair of nodes can potentially represent the same entity but needs further analysis .
more specifically , the approach relies on the notion of the connection strength c -LRB- u , v -RRB- between two nodes u and v in the graph g. the connection strength between u and v reflects how strongly u and v are connected to each other via paths that exist between them .
then , for any two nodes u and v we can characterize the connections among them with a path - type count vector tuv = -LRB- c1 , c2 , ... , cn -RRB- , where each ci is the number of paths of type ti that exist between u and v. the difference among the existing cs models is -LRB- a -RRB- in the way they classify paths into types , and -LRB- b -RRB- in the way they assign weights to path types , where both -LRB- a -RRB- and -LRB- b -RRB- are either already given and fixed or , alternatively , decided by the algorithm designer based on what is intuitively reasonable for a particular domain .
to test our approach , we will use a standard technique employed by data cleaning practitioners : we
