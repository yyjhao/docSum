this model simultaneously learns
let v denote the annotation vocabulary , t denote the training set of annotated images , and let j be an element of t. according to the previous section j is represented as a set of image regions rj = -LCB- r1 ... rn -RCB- along with the corresponding annotation wj e -LCB- 0,1 -RCB- v. we assume that the process that generated j is based on two distinct probability distributions .
now let ra = 191 ... 9na -RCB- denote the feature vectors of some image a , which is not in the training set t. similarly , let wb be some arbitrary subset of v. we would like to model p -LRB- ra , wb -RRB- , the joint probability of observing an image defined by ra together with annotation words wb .
given a new -LRB- un-annotated -RRB- image we can split it into regions ra , compute feature vectors 91 ... 9n for each region and then use equation 1 to determine what subset of vocabulary w \* is most likely to co-occur with the set of feature vectors : equation -LRB- 3 -RRB- arises out of placing a gaussian kernel over the feature vector 9i of every region of image j. each kernel is parametrized by the feature covariance matrix e. as a matter of convenience we assumed e = 0 i , where i is the identity matrix .
