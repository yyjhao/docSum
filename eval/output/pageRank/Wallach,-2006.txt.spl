such models typically fall into one of two categoriesthose that generate each word on the basis of some number of preceding words or word classes and those that generate words based on latent topic variables inferred from word correlations independent of the order in which the words appear .
in practice , the topics inferred using latent dirichlet allocation are heavily dominated by function words , such as in , that , of and for , unless these words are removed from corpora prior to topic inference .
in addition to exhibiting better predictive performance than either mackay and petos language model or latent dirichlet allocation , the topics inferred using the new model are typically less dominated by function words than are topics inferred from the same corpora using latent dirichlet allocation .
this dependence comes from the hyperparameter vector om , shared , in the case of the hierarchical dirichlet language model , between all possible previous word contexts j and
in latent dirichlet allocation , the latent topic for a given word is inferred using the identity of the word , the number of times the word has previously been assumed to be generated by each topic , and the number of times each topic has been used in the current document .
