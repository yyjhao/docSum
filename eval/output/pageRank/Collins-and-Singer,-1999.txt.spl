before describing the unsupervised case we first describe the supervised version of the algorithm : input to the learning algorithm : n labeled examples of the form
otherwise , label the training data with the combined spelling \/ contextual decision list , then induce a final decision list from the labeled examples where all rules -LRB- regardless of strength -RRB- are added to the decision list .
-LRB- blum and mitchell 98 -RRB- describe learning in the following situation : each example is represented by a feature vector , x drawn from a set of possible values -LRB- an instance space -RRB- x. the task is to learn a classification function f : x &gt; y where y is a set of possible labels , the features can be separated into two types : by this assumption , each element x e x can also be represented as -LRB- xi , x2 -RRB- e x1 x x2 .
the weak learner for two-class problems computes a weak hypothesis h from the input space into the reals -LRB- h : 2x -4 r -RRB- , where the sign4 of h -LRB- x -RRB- is interpreted as the predicted label and the magnitude i h -LRB- x -RRB- i is the confidence in the prediction : large numbers for i h -LRB- x -RRB- i indicate high confidence in the prediction , and numbers close to zero indicate low confidence .
