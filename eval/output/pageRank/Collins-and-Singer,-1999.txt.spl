before describing the unsupervised case we first describe the supervised version of the algorithm : input to the learning algorithm : n labeled examples of the form -LRB- xi , y -RRB- .
otherwise , label the training data with the combined spelling \/ contextual decision list , then induce a final decision list from the labeled examples where all rules -LRB- regardless of strength -RRB- are added to the decision list .
-LRB- blum and mitchell 98 -RRB- describe learning in the following situation : each example is represented by a feature vector , x drawn from a set of possible values -LRB- an instance space -RRB- x. the task is to learn a classification function f : x &gt; y where y is a set of possible labels , the features can be separated into two types : by this assumption , each element x e x can also be represented as -LRB- xi , x2 -RRB- e x1 x x2 .
each ht is a function that predicts a label -LRB- + 1 or 1 -RRB- on examples containing a particular feature xt , while abstaining on other examples
this modification brings the method closer to the dl-cotrain algorithm described earlier , and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples , preventing one label from dominating this deserves more theoretical investigation .
