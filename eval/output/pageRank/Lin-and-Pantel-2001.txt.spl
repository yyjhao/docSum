while richardson used paths as features to compute the similarity between words , we use words as features to compute the similarity of paths .
we make an assumption that this is an extension to the distributional hypothesis : extended distributional hypothesis : if two paths tend to occur in similar contexts , the meanings of the paths tend to be similar .
note that in information theory , mutual information refers to the mutual information between two random variables rather than between two events as used
given a path p , our algorithm for finding the most similar paths of p takes three steps : retrieve all the paths that share at least one feature with p and call them candidate paths .
for each of the paths p in the second column of table 5 , we ran the dirt algorithm to compute its top-40 most similar paths using the triple database .
there is very little overlap between the automatically generated paths and the paraphrases , even though the percentage of correct paths in dirt outputs can be quite high .
our experimental results show that the extended distributional hypothesis can indeed be used to discover very useful inference rules , many of which , though easily recognizable , are difficult for humans to recall .
