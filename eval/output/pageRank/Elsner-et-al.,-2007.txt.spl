to overcome this difficulty we simply normalize by force3 : the individual probabilities p -LRB- r + ej i r , rh -LRB- i _ 1 -RRB- , j -RRB- are calculated by counting situations in the training documents in which a known noun has history i h -LRB- i _ 1 -RRB- , j and fills slot
the emission model of each state is an instance of the relaxed entity grid model as described above , but in addition to conditioning on the role and history , we condition also on the state and on the particular set of lexical items lex -LRB- ki -RRB- which may be selected to fill the role : p -LRB- r + _ ej i r , ~ rh -LRB- i-1 -RRB- , j , qi , lex -LRB- ki -RRB- -RRB- .
the actual number of states found by the model depends mostly on the backoff constants , the ^ s -LRB- and , for pitman-yor processes , discounts -RRB- chosen for the emission models -LRB- the entity grid , non-entity word model and new noun model -RRB- , and is relatively insensitive to particular choices of prior for the other hyperparameters .
in the sentence ordering task , -LRB- lapata , 2003 ; barzilay and lee , 2004 ; barzilay and lapata , 2005 ; soricut and marcu , 2006 -RRB- , we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model .
