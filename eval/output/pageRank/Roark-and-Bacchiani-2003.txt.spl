gildea -LRB- 2001 -RRB- simply added the out-of-domain treebank to his in-domain training data , and derived a very small benefit for his high accuracy , lexicalized parser , concluding that even a large amount of out-of-domain data is of little use for lexicalized parsing .
in their language modeling approach , in-domain counts are mixed with the out-of-domain model , so that , if the number of observations within the domain is small , the outof-domain model is relied upon , whereas if the number of observations in the domain is high , the model will move toward a maximum likelihood -LRB- ml -RRB- estimate on the in - domain data alone .
while we will not be presenting empirical results for other parameter tying approaches in this paper , we should point out that the map framework is general enough to allow for other schema , which could potentially improve performance over
recall that a ~ c -LRB- a -RRB- times the out-of-domain model yields count merging , with a the ratio of out-of-domain to in-domain counts ; and ac -LRB- a -RRB- times the out-of-domain model yields model interpolation , with a the ratio of out-ofdomain to in-domain probabilities .
bacchiani and roark -LRB- 2003 -RRB- presented unsupervised map adaptation results for n-gram models , which use the same methods outlined above , but rather than using a manually annotated corpus as input to adaptation , instead use an automatically annotated corpus .
