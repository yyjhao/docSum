the algorithm builds two classifiers iteratively : each iteration involves minimization of a continuously differential function which bounds the number of examples on which the two classifiers disagree .
adaboost was first introduced in -LRB- freund and schapire 97 -RRB- ; -LRB- schapire and singer 98 -RRB- gave a generalization of adaboost which we will use in this paper .
-LRB- blum and mitchell 98 -RRB- describe learning in the following situation : each example is represented by a feature vector , x drawn from a set of possible values -LRB- an instance space -RRB- x. the task is to learn a classification function f : x &gt; y where y is a set of possible labels , the features can be separated into two types : by this assumption , each element x e x can also be represented as -LRB- xi , x2 -RRB- e x1 x x2 .
the coboost algorithm just described is for the case where there are two labels : for the named entity task there are three labels , and in general it will be useful to generalize the coboost algorithm to the multiclass case .
each ht is a function that predicts a label -LRB- + 1 or 1 -RRB- on examples containing a particular feature xt , while abstaining on other examples : we now briefly describe how to choose ht and at
