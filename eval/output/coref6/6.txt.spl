having trained on training set 5 ' , we can say
rather than assuming that the drawing of a classified example is an atomic operation -LRB- see valiant , 1984 ; blumer et al. , 1988 -RRB- , we may divide the operation into two steps : first , drawing an unclassified example from the distribution , and second , querying the classification of that point .
the backpropagation algorithm -LRB- rumelhart et al. , 1986 -RRB- is a supervised neural network learning technique , in that the network is presented with a training set of input \/ output pairs -LRB- x , t -LRB- x -RRB- -RRB- and learns to output t -LRB- x -RRB- when given input x .
if we draw at random over the whole domain , then the probability that an individual sample will reduce our error is a , as defined above , which decreases to zero as we draw more and more examples .
this creates a background bias over the domain that is weighted by the input distribution 19 : the networks that have the least error on these background patterns will be the ones that are the most specific according to v. in order to allow the network to converge on the actual training examples in spite of the background examples , we must balance the influence of background examples against that of the training data .
