if one accounts for the upper bound in table 2 , our methods do very well on those cases where a correct answer actually
even though we have justified equation 5 with reasonable independence assumptions , our four models may not be combined optimally for our pronoun resolution task , as the models are only approximations of the true distributions they are intended to represent .
ems role is to induce a probability distribution over candidates to maximize the likelihood of the -LRB- p , k -RRB- pairs observed in our training set : to improve our ability to generalize to future cases , we use a naive bayes assumption to state that the choices of pronoun and context are conditionally independent , given an antecedent .
the columns split the results into three cases : all pronouns with no exceptions ; all cases where the pronoun was found in a sentence containing no quotation marks -LRB- and therefore resembling the training data provided to em -RRB- ; and finally all pronouns excluded by the second case .
the majority of pronoun resolution approaches have thus far relied on manual intervention in the resolution process , such as using a manually-parsed corpus , or manually removing difficult non-anaphoric cases ; we follow -LRB- mitkov et al. -RRB- ' s approach -LRB- 2002 -RRB- with a fully-automatic pronoun resolution method .
