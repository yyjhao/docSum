here , in contrast , we use em to estimate the map parameters in a hierarchical generative model similar to lda -LRB- note that our m step explicitly takes into account the dirichlet priors on ^ , ^ -RRB- .
specifically , we compute -LRB- 1 -RRB- the expected number of topic transitions that ended in the topic z and -LRB- 2 -RRB- the expected number of co-occurrence of a word w with a topic z .
our model is similar to the lda model in tying together parameters of different documents via a hierarchical generative model , but unlike the lda model it does not assume documents are bags of words .
a somewhat related model is the aspect hmm model -LSB- 2 -RSB- , though
rather it assumes that the topics of words in a document form a markov chain , and that subsequent words are more likely to have the same topic .
unlike the lda and mixture of unigrams models , the htmm model -LRB- which allows infrequent topic transitions within a document -RRB- is no longer invariant to a reshuffling of the words .
applied to the htmm model , the latent variables are the topics zn and the variables ^ n that determine whether the topic n will be identical to topic n 1 or will it be drawn according to ^ d .
