for each parser , we calculated two scores , constituent effectiveness -LRB- fconst -RRB- and dependency effectiveness -LRB- fdep -RRB- against the original constituent trees in the treebank , and their dependency graph equivalents , respectively -LRB- see tables 1 and 2 -RRB- .
they report fdep scores of 88.5 and 92.0 for identifying the subjects and objects of verbs respectively , although it is not clear whether or not these relation types are defined as broadly as the categories we used above in the study of the verb ' induce ' , where the charniak-lease parser
this seems to reflect an architectural difference between the two parsers ; the version of the charniak parser tested here did not suffer any failures either , and neither did two previous versions that we tested in earlier experiments , whereas the later version of the bikel parser tested here failed twice -LRB- and was itself a bugfix release for a version that failed a staggering 440 times on our corpus -RRB- .
the stanford algorithm provides a de facto standard for comparing a variety of constituent parsers and treebanks at the dependency level ; if the dependency parser community were to adopt the same set of grammatical relations as standard , then native dependency parsers could be compared to constituent parsers and to biological treebanks fairly and transparently .
