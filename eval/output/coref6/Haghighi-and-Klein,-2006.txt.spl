we automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often .
an important characteristic of this domain -LRB- see figure 1 -LRB- a -RRB- -RRB- is that the hidden labels tend to be sticky , in that fields tend to consist of runs of the same label , as in figure 1 -LRB- c -RRB- , in contrast with part-of-speech tagging , where we rarely see adjacent tokens given the same label .
we believe the performance for chinese pos tagging is not as high as english for two reasons : the general difficulty of chinese pos tagging -LRB- tseng et al. , 2005 -RRB- and the lack of a larger segmented corpus from which to build distributional models .
we used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in smith and eisner -LRB- 2005 -RRB- : exact word
adding distributional similarity fea tures to our model -LRB- proto + sim -RRB- improves accuracy substantially , yielding 71.5 % , a 38.4 % error reduction over base .6 another feature of this domain that grenager et al. -LRB- 2005 -RRB- take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one .
