beal et al. -LRB- 2001 -RRB- proposes an inference algorithm based on particle filters , but we feel that in this case , the effects are relatively minor , so we approximate by treating the model as a standard hmm , using a fixed transition function based only on the training data .
since each entity in the document is new precisely once , we treat
the probability of a sentence is therefore dependent only on the entities : next , the model assumes that each entity ej appears in sentences and takes on syntactic roles independent of all the other entities .
to overcome this difficulty we simply normalize by force3 : the individual probabilities p -LRB- r + ej i r , rh -LRB- i _ 1 -RRB- , j -RRB- are calculated by counting situations in the training documents in which a known noun has history i h -LRB- i _ 1 -RRB- , j and fills slot r in the next sentence , versus situations where the slot r exists but is filled by some other noun .
in the sentence ordering task , -LRB- lapata , 2003 ; barzilay and lee , 2004 ; barzilay and lapata , 2005 ; soricut and marcu , 2006 -RRB- , we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model .
