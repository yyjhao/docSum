the most straightforward way of implementing this idea is in the form of a backoff scheme : if the n-gram count for an item in the corpus falls below a threshold 0 , the web is used to estimate the n-grams frequency , otherwise the corpus counts are used .8 note that this backoff scheme subsumes both a purely web-based model -LRB- 0 = 0 -RRB- and
these models then form the basis for the significance tests reported in table v. -RRB- as explained in section 2.5 , we also test two weakly supervised models that combine web counts and corpus counts using backoff and interpolation .
however , there is also evidence for the reliability of web counts : keller and lapata -LSB- 2003 -RSB- show that the counts generated by two search engines -LRB- google and altavista -RRB- are highly correlated with frequencies obtained from two standard corpora for english -LRB- the bnc and the north american newstext corpus -RRB- .
note that for certain tasks , the performance of a web baseline model might actually be sufficient , so that the effort of constructing a sophisticated supervised model and annotating the necessary training data can be avoided : recall that for three tasks , our web-based models outperformed the best model in the literature -LRB- for mt candidate selection , article generation , and compound interpretation , see table xx -RRB- .
