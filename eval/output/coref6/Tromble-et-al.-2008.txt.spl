the results show that when restricted to the 1000 - best list , lattice mbr performs slightly better than n-best mbr -LRB- with sentence bleu -RRB- on aren \/ enzh while n-best mbr is better on zhen .
for comparison , we also include results from lattice mbr when both hypothesis and evidence spaces are identical : either the full lattice or the 1000-best list -LRB- from tables 2 and 3 -RRB- .
given such a loss function l -LRB- e , e ' -RRB- between an automatic translation e ' and the reference e , and an underlying probability model p -LRB- eif -RRB- , the mbr decoder has the following form -LRB- goel and byrne , 2000 ; kumar and byrne , 2004 -RRB- : we are interested in performing mbr decoding under a sentence-level bleu score -LRB- papineni et al. , 2001 -RRB- which behaves like a gain function : it varies between 0 and 1 , and a larger value reflects a higher similarity .
in our setting , this weight will imply the posterior probability of the translation e given the source sentence f : because a lattice may represent a number of candidates exponential in the size of its state set , it is often impractical to compute the mbr decoder -LRB- equation 1 -RRB- directly .
we now show how the lattice mbr decision rule -LRB- equation 6 -RRB- can be implemented using weighted finite state automata -LRB- mohri
