since the objective was to assess how the method scaled we used our preferred phrasetable smoothing technique of zens-ney and separated our corpus into two phrase - tables , one based on the un corpus and the other based on the best of the remaining parallel corpora available to us .
in addition , a number of different phrasetable smoothing algorithms were used
since the contingency table with c -LRB- s , ~ t -RRB- = 1 having the greatest significance -LRB- lowest p-value -RRB- is the 1-1-1 table , using the threshold of a + e can be used to exclude all of the phrase pairs occurring exactly once -LRB- c -LRB- 9 , ~ t -RRB- = 1 -RRB- .
the idea behind significance pruning of phrasetables is that not all of the phrase pairs in a phrasetable are equally supported by the data and that many of the weakly supported pairs could be removed because : the chance of them occurring again might be low , and their occurrence in the given corpus may be the result of an artifact -LRB- a combination of effects where several estimates artificially compensate for one another -RRB- .
to test the effects of significance pruning on larger corpora , a series of experiments was run on a much larger corpus based on that distributed for mt06 chineseenglish -LRB- nist mt , 2006 -RRB- .
