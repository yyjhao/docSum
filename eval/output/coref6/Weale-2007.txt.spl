in this paper , we have introduced a method for document categorization using wikipedia as a large - scale knowledge base for information about named entities .
in this paper , we propose to use wikipedia to extract the hidden information in entities for more accurate classification than would be able if we used surface features alone .
the resulting classification vectors are based on all categories found somewhere in our training set -LRB- rather than all possible categories -RRB- .
we then reconfigured our training examples based on the results of the entity recognition process , resulting in a 28 positive \/ 12 negative training split .
additionally , they observe that since their algorithm is language independent , additional languages can be added by simply running their algorithm over the appropriate wikipedia language set .
going back to the copper example , if the
in our evaluation , we decided to try two different types of classifiers : support vector machines and decision trees .
to do this , we run an named entity recognizer over each document in the corpus and extract the named entities to be used for article matching .
our toy corpus was used to demonstrate the initial concept , but we need to see how this might work in a more general classification context .
