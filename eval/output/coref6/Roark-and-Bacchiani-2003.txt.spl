in their language modeling approach , in-domain counts are mixed with the out-of-domain model , so that , if the number of observations within the domain is small , the outof-domain model is relied upon , whereas if the number of observations in the domain is high , the model will move toward a maximum likelihood -LRB- ml -RRB- estimate on the in - domain data alone .
hwa -LRB- 1999 -RRB- and gildea -LRB- 2001 -RRB- looked at adapting parsing models trained on large amounts of annotated data from outside of the domain of interest -LRB- out-of-domain -RRB- , through the use of a relatively small amount of in-domain annotated data .
while we will not be presenting empirical results for other parameter tying approaches in this paper , we should point out that the map framework
this may be because of the same issue as with the brown corpus , namely that the optimal ratio of in-domain to out-of-domain is not 1 and the smoothing parameters need to be tuned to the new domain ; or it may be because the new domain has a million words of training data , and hence has less use for out-of-domain data .
in our case , we use the parsing model trained on out-of-domain data , and output a set of candidate parse trees for the strings in the in-domain corpus , with their normalized scores .
