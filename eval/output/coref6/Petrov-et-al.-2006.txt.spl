we ran our experiments on the wall street journal -LRB- wsj -RRB- portion of the penn treebank using the standard setup : we trained on sections 2 to 21 , and we used section 1 as a validation set for tuning model hyperparameters .
for instance , matsuzaki et al. -LRB- 2005 -RRB- start by annotating their grammar with the identity of the parent and sibling , which are observed -LRB- i.e. not latent -RRB- , before adding latent annotations .4 if these manual annotations are good , they reduce the search space for em by constraining it to a smaller region .
inspecting a
we found that merging 50 % of the newly split symbols dramatically reduced the grammar size after each splitting round , so that after 6 sm cycles , the grammar was only 17 % of the size it would otherwise have been -LRB- 1043 vs. 6273 subcategories -RRB- , while at the same time there was no loss in accuracy -LRB- figure 3 -RRB- .
a particularly clean example of a split correlating external with internal contexts is the inverted sentence category -LRB- sinv -RRB- , which has only two subsymbols , one which usually has the root symbol as its parent -LRB- and which has sentence final puncutation as its last child -RRB- , and a second subsymbol which occurs in embedded contexts -LRB- and does not end in punctuation -RRB- .
