this domain is experimentally convenient because there are large on-line collections of such reviews , and because reviewers often summarize their overall sentiment with a machine-extractable rating indicator , such as a number of stars ; hence , we did not need to hand-label the data for supervised learning or evaluation purposes .
we also note that turney -LRB- 2002 -RRB- found movie reviews to be the most difficult of several domains for sentiment classification , reporting an accuracy of 65.83 % on a 120 - document set -LRB- random-choice performance : 50 % -RRB- .
these experiments also provide us with baselines for experimental comparison ; in particular , the third baseline of 69 % might actually be considered somewhat difficult to beat , since it was achieved by examination of the test data -LRB- although our examination was rather cursory ; we do 4later experiments using these words as features for machine learning methods did not yield better results .
its estimate of p -LRB- c i d
because training maxent is expensive in the number of features , we limited consideration to -LRB- 1 -RRB- the 16165 unigrams appearing at least four times in our 1400 - document corpus -LRB- lower count cutoffs did not yield significantly different results -RRB- , and -LRB- 2 -RRB- the 16165 bigrams occurring most often in the same data -LRB- the selected bigrams all occurred at least seven times -RRB- .
