first , we create clusters via complete-link clustering , measuring sentence similarity by the cosine metric using word bigrams as features -LRB- figure 1 shows example output -RRB- .4 then , given our knowledge that documents may sometimes discuss new and \/ or irrelevant content as well , we create an etcetera cluster by merging together all clusters containing fewer than sentences , on the assumption that such clusters consist of outlier sentences .
we demonstrate this point by utilizing content models to select appropriate sentence orderings : we simply use a content model trained on documents from the domain of interest , selecting the ordering among all the presented candidates that the content model assigns the highest probability to .
we employ an iterative re-estimation procedure that alternates between -LRB- 1 -RRB- creating clusters of text spans with similar word distributions to serve as representatives of within-document topics , and -LRB- 2 -RRB- computing
the parameter values were selected to optimize system performance 6if there are more than sentences , we prioritize them by the summarization probability of their v-topic 's state ; we break any further ties by order of appearance in the document .
5.2 parameter estimation our training algorithm has four free parameters : two that indirectly control the number of states in the induced content model , and two parameters for smoothing bigram probabilities .
