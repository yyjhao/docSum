from the previous example , we form the pattern x is the largest y , where we replace the two words jaguar and cat by two wildcards x and y. our contributions in this paper are two fold : we propose an automatically extracted lexico-syntactic patterns-based approach to compute semantic similarity using text snippets obtained from a web search engine .
one must consider the page counts not just for the query p and q , but also for the individual words p and q to assess semantic similarity between p and q. we modify four popular co-occurrence measures ; jaccard , overlap -LRB- simpson -RRB- , dice , and pmi -LRB- point-wise mutual information -RRB- , to compute semantic similarity using page counts .
this function returns a vector of patterns where each element is the normalized frequency of the corresponding pattern in the snippets for the query a b. we append similarity scores calculated using page counts in section 3.2 to create the final feature vector f for the word - pair -LRB- a , b -RRB- .
measuring semantic similarity between named entities is vital in many applications such as query expansion -LSB- 36 -RSB- , entity disambiguation -LRB- e.g. namesake disambiguation -RRB- and community mining -LSB- 19 -RSB- .
let us assume these wildcards to be x and y. for each snippet d in the set of snippets d returned by getsnippets , function getngrams
