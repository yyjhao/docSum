one source of variation in the data that we want to ignore in order to
when we first implemented a disambiguation mechanism of the kind described above , an initial set of scaling factors was chosen by hand according to knowledge of how the particular raw preference functions were computed and introspection about the ' strength ' of the functions as indicators of preference .
as mentioned earlier , the preference score is a weighted sum of a set of preference functions : each preference function fb takes a complete qlf representation qi as input , returning a numerical score sib , the overall preference score being computed by summing over the product of function scores with their associated scaling factors cb .
this least-squares set of scaling factors achieves quite good disambiguation performance -LRB- see section 4 below -RRB- but is not truly optimal because of the inherent nonlinearity of the goal , which is to maximize the proportion of sentences for which a correct qlf is selected , rather than to approximate training scores -LRB- even relativized ones .
from these five functions on triples we define five semantic collocation preference functions applied to qlfs , in each case by averaging over the result of applying the function to each triple derived from a qlf .
