we will introduce the shallow semantic analysis -LRB- section 2 -RRB- and the deep semantic analysis -LRB- section 3 -RRB- , present the results of our two runs -LRB- section 4 -RRB- , and discuss them -LRB- section 5 -RRB- .
this way the models they output are generally minimal models ; in other words , the models do not contain entities that are not mentioned in the input
we perform model building using two different model builders : we use paradox to find the size of the domain , and then use mace to construct a minimal model giving that domain size .
therefore , we split the test data into three subsets : testshallow contains examples which could not be handled via the deep analysis because the examples either could not be parsed or because the theorem prover discovered an inconsistency of t or t \/ h with the background knowledge .
if vampire finds a proof for 1 , it is very likely that t entails h. if vampire finds a proof for 3 or 4 , then we are -LRB- unfortunately -RRB- dealing with inconsistent background knowledge and there is nothing we can say about the relationship between t and h -LRB- however , it could be the case that t + h is inconsistent , which would mean that t does not entail h -RRB- .
