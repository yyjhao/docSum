for this purpose , we have developed a system called groundhog , which relies on our ability to derive a variety of lexico-semantic information from text , including information about named entities , coreference , and syntactic and semantic dependencies .
since the headlines and first sentences of newswire texts are often used to synopsize the content of a document , we found that most extracted
four classes of features are extracted : -LRB- 1 -RRB- alignment features , which compare properties of aligned constituents , -LRB- 2 -RRB- dependency features , which compare entities and predicates using dependencies identified by a semantic parser , -LRB- 3 -RRB- paraphrase features , which determine whether passages extracted from the text and hypothesis match acquired paraphrases , and -LRB- 4 -RRB- semanticfeatures , which contrast semantic values assigned to predicates in each example sentence .
since performing this alignment requires access to large amounts of training data , this classifier is trained using two large corpora of positive and negative examples of textual entailment that we extracted from newswire corpora .
these annotations were used to train a hill - climber that was used to annotate a larger set of 450,000 alignment pairs selected at random from the training corpora described in section 4 that was then used to train a maximum entropy-based classifier that was used on the 2006 test set .
