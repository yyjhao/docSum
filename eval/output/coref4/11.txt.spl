in the following four sections , we describe the changes we have made to our system for this year : question classification -LRB- section 3 -RRB- , storing data with multi-dimensional markup -LRB- section 4 -RRB- , and probabilistic answer processing -LRB- sections 5 and 6 -RRB- .
we therefore decided on using three different types of question classes : a table type that linked the question to an available table column -LRB- 17 classes -RRB- , a coarse-grained type that linked the question to the types recognized by our named-entity recognizer -LRB- 7 classes -RRB- , and a fine-grained type that linked the question to wordnet synsets -LRB- 166 classes -RRB- .
in order to make it possible for our answer re-ranking module -LRB- described in section 6 -RRB- to rank answers from different streams , we took advantage of answer patterns from previous editions of clef qa to estimate the probability that an answer from a given stream with a given confidence score is correct .
for answers to questions with the following answer types only the very basic well-formedness checks noted are additionally performed : for questions expecting named entities , dates , or numeric answers , more significant
based on this analysis , we have created the following list of future work : as the answers to the first two questions show , our system still prefers infrequent long answers over frequent short ones .
