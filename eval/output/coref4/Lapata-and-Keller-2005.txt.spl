the most straightforward way of implementing this idea is in the form of a backoff scheme : if the n-gram count for an item in the corpus falls below a threshold 0 , the web is used to estimate the n-grams frequency , otherwise the corpus counts are used .8 note that this backoff scheme subsumes both a purely web-based model -LRB- 0 = 0 -RRB- and a purely corpus-based model -LRB- 0 = k , where k is the largest observed corpus count -RRB- .
we
they presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts -LRB- a -RRB- correlate with frequencies obtained from a carefully edited , balanced corpus such as the 100m words british national corpus -LRB- bnc -RRB- , -LRB- b -RRB- correlate with frequencies recreated using smoothing methods in the case of unseen bigrams , -LRB- c -RRB- reliably predict human plausibility judgments , and -LRB- d -RRB- yield state-of-the-art performance on pseudo-disambiguation tasks .
note that for certain tasks , the performance of a web baseline model might actually be sufficient , so that the effort of constructing a sophisticated supervised model and annotating the necessary training data can be avoided : recall that for three tasks , our web-based models outperformed the best model in the literature -LRB- for mt candidate selection , article generation , and compound interpretation , see table xx -RRB- .
