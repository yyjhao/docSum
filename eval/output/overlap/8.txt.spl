we begin by reviewing the use of
chelba and acero -LSB- 5 -RSB- use out-of-task labeled examples in logistic regression training of a text capitalizer , and use the resulting map estimate as the mode vector of a bayesian prior for training with in-task examples .
another simple baseline is to create x copies of the prior knowledge text for a class and add these copies to the training data as additional positive examples -LRB- dk examples in table 1 -RRB- , as in some relevance feedback approaches .
two of our methods look not just at the domain knowledge text for the target class , but at the texts for other classes , in order to determine how significant to the target class each word in its domain knowledge text is .
our primary hypothesis was that using domain knowledge texts would greatly improve classifier effectiveness when few training examples are available , and not hurt effectiveness with large training sets .
table 2 summarizes the types of domain knowledge used , and the number of domain knowledge texts used to compute significance values for the var \/ tfidf and mode \/ tfidf methods .
method dk examples -LRB- using domain knowledge texts as artificial positive examples -RRB- had little impact on any learning algorithm with these large training sets .
