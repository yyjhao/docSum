numerous studies show that effective text classifiers can be produced by supervised learning methods , including support vector machines -LRB- svms -RRB- -LSB- 11 , 14 , 33 -RSB- , regularized logistic regression -LSB- 9 , 33 -RSB- , and other approaches -LSB- 14 , 27 , 33 -RSB- .
several recent papers have modified learning approaches naive bayes -LSB- 13 , 16 -RSB- , logistic regression -LRB- fit with a boosting - style algorithm -RRB- -LSB- 25 -RSB- , and svms -LSB- 31 -RSB- to use domain knowledge in text categorization .
chelba and acero -LSB- 5 -RSB- use out-of-task labeled examples in logistic regression training of a text capitalizer , and use the resulting map estimate as the mode vector of a bayesian prior for training with in-task examples .
some papers explore several values for the prior variance -LSB- 15 , 33 -RSB- , others use a single value but do not say how it was chosen -LSB- 19 , 32 -RSB- , and others choose the variance by cross-validation on the training set -LSB- 9 -RSB- .
table 2 summarizes the types of domain knowledge used , and the number of domain knowledge texts used to compute significance values
we note that our macroaveraged f1 for svms on modapte top 10 -LRB- 86.55 -RRB- is similar to that found by wu &amp; srihari -LRB- approximately 83.5 on a non-random sample of 1,024 training examples , from the graph in figure 3 -LSB- 31 -RSB- -RRB- and joachims -LRB- 82.5 with all 9,603 training examples , computed from his figure 2 -LSB- 11 -RSB- -RRB- .
