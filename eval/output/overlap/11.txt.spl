we therefore decided on using three different types of question classes : a table type that linked the question to an available table column -LRB- 17 classes -RRB- , a coarse-grained type that linked the question to the types recognized by our named-entity recognizer -LRB- 7 classes -RRB- , and a fine-grained type that linked the question to wordnet synsets -LRB- 166 classes -RRB- .
in order to make it possible for our answer re-ranking module -LRB- described in section 6 -RRB- to rank answers from different streams , we took advantage of answer patterns from previous editions of clef qa to estimate the probability that an answer from a given stream with a given confidence score is correct .
each answer in the cluster is checked for well-formedness and well-typedness -LRB- line 5 : section 6.3 -RRB- : ans.well formed is a boolean value indicating whether some part of the original answer is well-formed -LRB- ans.string is the corresponding part of the
for answers to other questions , we use the same ratio for ill-formed answers as we do for ill-typed answers -LRB- 0.34 -RRB- , but we do not compute any update for well-formed answers -LRB- i.e. , we update with a ratio of 1.0 -RRB- .
for answers to questions with the following answer types only the very basic well-formedness checks noted are additionally performed : for questions expecting named entities , dates , or numeric answers , more significant wellformedness and well-typedness checks are performed .
