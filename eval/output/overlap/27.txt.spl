sch olkopf -LSB- 14 -RSB- showed that the prior knowledge can be incorporated with the appropriate kernel function , and fung -LSB- 4 -RSB- showed prior knowledge in the form of multiple polyhedral sets can be used with a reformulation of svm .
second
given the true training dataset and pseudo training dataset , we now have two possibly conflicting goals in minimizing the empirical risk when constructing a predictor : -LRB- 1 -RRB- fit the true training dataset , and -LRB- 2 -RRB- fit the pseudo training dataset and thus fit the prior knowledge .
let the first m training examples be the labeled examples , and the rest be the pseudo examples , the objective function of primal problem is given : here the functionality of the parameter c is the same as the standard svm to control the balance between the model complexity and training error .
to test the effectiveness of the proposed way of incorporating prior knowledge , we compare the performance of wmsvm with prior knowledge against svm without such knowledge , particularly when the true labeled dataset is small .
the performance of wmsvm with the prior knowledge on reuters is comparable to that of transductive svm -LSB- 8 -RSB- , but the training time is much less as only one iteration of svm training is needed .
