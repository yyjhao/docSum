a content model is an hmm in which each state corresponds to a distinct topic and generates sentences relevant to that topic according to a state-specific language model note that standard - gram language models can therefore be considered to be degenerate -LRB- single-state -RRB- content models .
we can use the forward algorithm to efficiently compute the generation probability assigned to a document by a content model and the viterbi algorithm to quickly find the most likely content - model state sequence to have generated a given document ; see rabiner -LRB- 1989 -RRB- for details .
we demonstrate this point by utilizing content models to select appropriate sentence orderings : we simply use a content model trained on documents from the domain of interest , selecting the ordering among all the presented candidates that the content model assigns the highest probability to .
given a content model acquired from the full articles using the method described in section
content 2.67 72 % 0.81 earthquakes lapata -LRB- n \/ a -RRB- 24 % 0.48 bigram 485.16 4 % 0.27 content 3.05 48 % 0.64 clashes lapata -LRB- n \/ a -RRB- 27 % 0.41 bigram 635.15 12 % 0.25 content 15.38 38 % 0.45 drugs lapata -LRB- n \/ a -RRB- 27 % 0.49 bigram 712.03 11 % 0.24 content 0.05 96 % 0.98 finance lapata -LRB- n \/ a -RRB- 18 % 0.75 bigram 7.44 66 % 0.74 content 10.96 41 % 0.44 accidents lapata -LRB- n \/ a -RRB- 10 % 0.07 bigram 973.75 2 % 0.19 table 2 : ordering results -LRB- averages over the test cases -RRB- .
