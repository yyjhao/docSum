given the parameters a and r , the joint distribution of a topic mixture 0 , a set of n topics z , and a set of n words w is given by : we refer to the latent multinomial variables in the lda model as topics , so as to exploit text-oriented intuitions , but we make no epistemological claims regarding these latent
the plsi model , illustrated in figure 3c , posits that a document label d and a word wn are conditionally independent given an unobserved topic z : the plsi model attempts to relax the simplifying assumption made in the mixture of unigrams model that each document is generated from only one topic .
however , variational inference provides us with a tractable lower bound on the log likelihood , a bound which we can maximize with respect to a and r. we can thus find approximate empirical bayes estimates for the lda model via an alternating variational em procedure that maximizes a lower bound with respect to the variational parameters y and 0 , and then , for fixed values of the variational parameters , maximizes the lower bound with respect to the model parameters a and r. we provide a detailed derivation of the variational em algorithm for lda in appendix a. 4 .
