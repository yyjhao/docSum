these include a variety of bayesian classifiers -LSB- golding 1995 ; golding and schabes 1996 -RSB- , decision lists -LSB- golding 1995 -RSB- transformation-based learning -LSB- mangu and brill 1997 -RSB- , latent semantic analysis -LRB- lsa -RRB- -LSB- jones and martin 1997 -RSB- , multiplicative weight update algorithms -LSB- golding and roth 1999 -RSB- , and augmented mixture models -LSB- cucerzan and yarowsky 2002 -RSB- .
subsequent work has used -LRB- v , n1 , p , n2 -RRB- tuples extracted from the penn treebank to train supervised models including a maximum entropy model -LSB- ratnaparkhi et al. 1993 -RSB- , a back-off model -LSB- collins and brooks 1995 -RSB- , transformation-based -LSB- brill and resnik 1994 -RSB- , and memory-based learning -LSB- zavrel et al. 1997 -RSB- .
the training \/ test set therefore contained solely head words -LRB- e.g. , the vp -LSB- -LSB- joined -LSB- the board -RSB- -RSB- -LSB- as a nonexecutive director -RSB- -RSB- would give the tuple -LRB- joined , board , as , director -RRB- -RRB- and the majority of previous models rely primarily on head words for disambiguating pp attachment .
for two tasks we found that a combined model outperforms a straight web model , in the sense that the combined model was significantly better than the corpus-based model or the model in the literature , while the straight web model failed to achieve a significant difference -LRB- see the asterisks in table xx -RRB- .
note that for certain tasks , the performance of a web baseline model might actually be sufficient , so that the effort of constructing
