within each application setting the annotators selected both positive entailment examples -LRB- true -RRB- , where t is judged to entail h , as well as negative examples -LRB- false -RRB- , where entailment does not hold -LRB- a 50 % -50 % split -RRB- .
annotators selected a text t from some news story which includes a certain relation , for which a paraphrase acquisition system produced a set of paraphrases -LRB- see acknowledgements -RRB- .
the remaining examples were considered as the gold standard for evaluation , split to 567 examples in the development set and 800 in the test set , and evenly split to true \/ false examples .
in order to encourage systems and methods which do not cover all entailment phenomena we allowed submission of partial coverage results , for only part of the test examples .
this is slightly different from the common use of average precision measures in ir and qa , in which
since the dataset was balanced in terms of true and false examples , a system that uniformly predicts true -LRB- or false -RRB- would achieve an accuracy of 50 % which constitutes a natural baseline .
a system which does well in recognizing when entailment does not hold would do just as well in terms of accuracy and cws as a system tailored to recognize true examples .
