in its most general form , our model assumes : -LRB- 1 -RRB- a joint distribution over entities , so that a document that mentions president kennedy is more likely to mention oswald or white house than roger clemens ; -LRB- 2 -RRB- an author model , which makes sure that at least one mention of a name in a document is easily identifiable -LRB- after all , that is the authors goal -RRB- , and then generates other mentions via -LRB- 3 -RRB- an appearance model , governing how mentions are transformed from the representative mention .
in addition to further studies of the discriminative model , including going beyond the current noisy supervision -LRB- given at a global annotation level , although learning is done locally -RRB- , exploring how much data is needed for a supervised model to perform as well as the unsupervised model , and whether the initialization of the unsupervised model can gain from supervision , there are several other critical issues we would like to address from the robust reading perspective .
these include -LRB- 1 -RRB- incorporating more contextual information -LRB- like time and place -RRB- related to the target entities , both to support a better model and to allow temporal tracing of entities ; -LRB- 2 -RRB- studying an incremental approach to learning the model and -LRB- 3 -RRB- developing a unified technique based on
