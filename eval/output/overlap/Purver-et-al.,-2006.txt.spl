we use gibbs sampling , drawing the topic assignment for each word , zu , z , conditioned on all
however , while hmm approaches allow a segmentation of the data by topic , they do not allow adaptively combining different topics into segments : while a new segment can be modelled as being identical to a topic that has already been observed , it can not be modelled as a combination of the previously observed topics .1 note that while -LRB- imai et al. , 1997 -RRB- s hmm approach allows topic mixtures , it requires supervision with hand-labelled topics .
of these lists , 40 contained the most indicative words for each of the 10 topics from different models : the topic segmentation model ; a topic model that had the same number of segments but with fixed evenly spread segmentation boundaries ; an equivalent with randomly placed segmentation boundaries ; and the hmm .
interestingly , using an even distribution of boundaries but allowing the topic model to infer topics performs similarly well with even segmentation , but badly with random segmentation topic quality is thus not very susceptible to the precise segmentation of the text , but does require some reasonable approximation -LRB- on icsi data , an even segmentation gives a pk of about 50 % , while random segmentations can do much worse -RRB- .
