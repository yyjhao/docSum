in the offline tm training approach , where one enumerates all the source phrases in the bilingual corpus and extracts their possible translations , it is clear that one better not to store translations for phrases longer than 3 words , otherwise the decoder is not able to load the phrase translation model during decoding .
we now modify the ibm1 alignment model by not summing the lexicon probabilities of all target words , but by restricting this summation in the following ways : for words inside the source phrase we sum only over the probabilities for words inside the target phrase candidate , and for words outside of the source phrase we sum only over the probabilities for the words outside the target phrase candidates ; the position alignment probability , which for the standard ibm1 alignment is 1 \/ i , where i is the number of words in the target sentence , is modified to 1 \/ l inside the source phrase and to 1 \/ -LRB- i-l -RRB- outside the source phrase .
the primary motivation of the fast substring searching algorithm was to efficiently locate all substrings of a testing sentence in the training corpus , so that the alignment program asp can extract the corresponding translations from the target side of the bilingual
