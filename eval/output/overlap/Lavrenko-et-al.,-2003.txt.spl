our model computes a joint probability of image features over different regions in an image using a training set and uses this joint probability to annotate and retrieve images .
more formally , we propose a statistical generative model to automatically learn the semantics of images - that is , for annotating and retrieving images based on a training set of images .
given a training set of images with annotations , we show that probabilistic models allow us to predict the probability of generating a word given the features computed over different regions in an image .
it is almost an order of magnitude better -LRB- in terms of mean precision -RRB- than a model based on word-blob co-occurrence model , more than two and a half times better
their translation model applies one of the classical statistical machine translation models to translate from the set of keywords of an image to the set of blobs forming the image .
image pixels are produced by first picking a set of i.i.d. feature vectors , then generating image regions from the feature vectors , and finally stacking the regions on top of each other .
we divided the dataset into 3 parts - with 4,000 training set images , 500 evaluation set images and 500 images in the test set .
