the simplification assumed above that coreference holds for
this hope is notwithstanding the fact that the algorithm is based on a simplifying assumption that each pronoun is associated with exactly one correct antecedent that is clearly false for a variety of reasons : -LRB- i -RRB- there will be cases in which there is more than one coreferential antecedent in the search window , all but one of which will get labeled as not coreferential during any given iteration , -LRB- ii -RRB- there will be cases in which the -LRB- perhaps only -RRB- correct antecedent was misparsed or incorrectly weeded out by hard constraints , and thus not seen by the learning algorithm -LRB- presumably some of the unambiguous cases identified in step 1c will be incorrect because of this -RRB- , and -LRB- iii -RRB- some of the pronouns found will not even be referential , e.g. pleonastic pronouns .
to allow for the closest comparison with our supervised system , we opted to train the system with the same number of pronouns that we had in our supervised training set -LRB- 2773 -RRB- , and sought to have approximately the same ratio of positive to negative training instances , which meant randomly including one-fifth of the pronouns in the raw data that had more than one possible antecedent -LRB- see step 1d -RRB- .
