for example , japanese has no distinct l
for consistency , we continue to print written english word sequences in italics -LRB- golf ball -RRB- , english sound sequences in all capitals -LRB- g aa l f b ao l -RRB- , japanese sound sequences in lower case -LRB- goruhubooru -RRB- and katakana sequences naturally -LRB- = ' -RRB- 1 -RRB- .
this is an inherently information-losing process , as english r and l sounds collapse onto japanese r , the 14 english vowel sounds collapse onto the 5 japanese vowel sounds , etc .
for each glossary entry , we converted english words into english sounds using the model described in the previous section , and we converted katakana words into japanese sounds using the model we describe in the next section .
next comes the poo model , which produces a 28-state \/ 31-arc wfsa whose highest-scoring sequence is : this english string is closest phonetically to the japanese , but we are willing to trade phonetic proximity for more sensical english ; we rescore this vvfsa by composing it with p -LRB- w -RRB- and extract the best translation : other section 1 examples -LRB- aasudee and robaato shyoon renaado -RRB- are translated correctly as earth day and robert sean leonard .
as we have presented it , our hypothetical human transliterator produces japanese sounds from english sounds only , without regard for the original english spelling .
