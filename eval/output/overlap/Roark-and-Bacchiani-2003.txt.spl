gildea -LRB- 2001 -RRB- simply added the out-of-domain treebank to his in-domain training data , and derived a very small benefit for his high accuracy , lexicalized parser , concluding that even a large amount of out-of-domain data is of little use for lexicalized parsing .
in their language modeling approach , in-domain counts are mixed with the out-of-domain model , so that , if the number of observations within the domain is small , the outof-domain model is relied upon , whereas if the number of observations in the domain is high , the model will move toward a maximum likelihood -LRB- ml -RRB- estimate on the in - domain data alone .
recall that a ~ c -LRB- a -RRB- times the out-of-domain model yields count merging , with a the ratio of out-of-domain to in-domain counts ; and ac -LRB- a -RRB- times the out-of-domain model yields model interpolation , with a the ratio of out-ofdomain to in-domain probabilities .
this may be because of the same issue as with the brown corpus , namely that the optimal ratio of in-domain to out-of-domain is not 1 and the smoothing parameters need to be tuned to the new domain ; or it may be because the new domain has a million words of training data , and hence has less use for out-of-domain data .
bacchiani and roark -LRB- 2003 -RRB- presented unsupervised map adaptation results for n-gram models , which use the same methods outlined
