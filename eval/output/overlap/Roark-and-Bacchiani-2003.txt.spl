gildea -LRB- 2001 -RRB- simply added the out-of-domain treebank to his in-domain training data , and
if the amount of in - domain -LRB- adaptation -RRB- data is large , the mode of the posterior distribution is mostly defined by the adaptation sample ; if the amount of adaptation data is small , the mode will nearly coincide with the mode of the prior distribution .
in their language modeling approach , in-domain counts are mixed with the out-of-domain model , so that , if the number of observations within the domain is small , the outof-domain model is relied upon , whereas if the number of observations in the domain is high , the model will move toward a maximum likelihood -LRB- ml -RRB- estimate on the in - domain data alone .
recall that a ~ c -LRB- a -RRB- times the out-of-domain model yields count merging , with a the ratio of out-of-domain to in-domain counts ; and ac -LRB- a -RRB- times the out-of-domain model yields model interpolation , with a the ratio of out-ofdomain to in-domain probabilities .
this may be because of the same issue as with the brown corpus , namely that the optimal ratio of in-domain to out-of-domain is not 1 and the smoothing parameters need to be tuned to the new domain ; or it may be because the new domain has a million words of training data , and hence has less use for out-of-domain data .
