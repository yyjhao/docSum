the latent dirichlet allocation -LRB- lda -RRB- model -LSB- 3 -RSB- introduces a more consistent probabilistic approach as it ties the parameters of all documents via a hierarchical generative model .
the model posits that words are either generated from topics that are randomly drawn from the topic mixture of the document or from the syntactic classes that are drawn from the previous syntactic class .
only the latent variables of the syntactic classes are treated as a sequence with local dependencies while latent assignments of topics are treated similar to the lda model , namely topics extraction is not benefited from the additional information conveyed in the structure of words .
these are the bigram topic model -LSB- 15 -RSB- , the lda collocation model and the topical n-grams model -LSB- 16 -RSB- .
our model is similar to the lda model in tying together parameters of different documents via a hierarchical generative model , but unlike the lda model it does not assume documents are bags of words .
rather it assumes that the topics of words in a document form a markov chain , and that
unlike the lda and mixture of unigrams models , the htmm model -LRB- which allows infrequent topic transitions within a document -RRB- is no longer invariant to a reshuffling of the words .
