each component divides words into finer groups according to a different criterion : the function words are divided into syntactic classes , and the content words are divided into semantic topics .
in addition
by considering only one of the factors influencing the words that appear in documents , these approaches are forced to assess all words on a single criterion : an hmm will group nouns together , as they play the same syntactic role even though they vary across contexts , and a topic model will assign determiners to topics , even though they bear little semantic content .
in addition to running the composite model with t = 200 and c = 20 , we examined two special cases : t = 200 , c = 2 , being a model where the only hmm classes are the start \/ end and semantic classes , and thus equivalent to latent dirichlet allocation -LRB- lda ; -LSB- 6 -RSB- -RRB- ; and t = 1 , c = 20 , being an hmm in which the semantic class distribution does not vary across documents , and simply has a different hyperparameter from the other classes .
the model cleanly separates words that play syntactic and semantic roles , in sharp contrast to the results of the lda model , also shown in the figure , where all words are forced into topics .
