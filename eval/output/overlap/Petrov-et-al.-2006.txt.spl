for example , even beginning with an x-bar grammar -LRB- see section 1.1 -RRB- with 98 symbols , our best grammar , using 1043 symbols , achieves a test set f , of 90.2 % .
therefore , this initialization is the absolute minimum starting grammar that includes the evaluation nonterminals -LRB- and maintains separate grammar symbols for each of them -RRB- .5 it is a very compact grammar : 98 symbols , 6 236 unary rules , and 3840 binary rules .
we found that merging 50 % of the newly split symbols dramatically reduced the grammar size after each splitting round , so that after 6 sm cycles , the grammar was only 17 % of the size it would otherwise have been -LRB- 1043 vs. 6273 subcategories -RRB- , while at the same time there was no loss in accuracy -LRB- figure 3 -RRB- .
inspecting a large grammar by hand is difficult , but fortunately , our baseline grammar has less than 100 nonterminal symbols , and even our most complicated grammar has only 1043 total -LRB- sub -RRB- symbols .
a particularly clean example of a split correlating external with internal contexts is the
hierarchical split \/ merge training enables us to learn compact but accurate grammars , ranging from extremely compact -LRB- an fl of 78 % with only 147 symbols -RRB- to extremely accurate -LRB- an fl of 90.2 % for our largest grammar with only 1043 symbols -RRB- .
