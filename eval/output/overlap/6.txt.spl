most neural network generalization problems are studied only with respect to random sampling : the training examples are chosen at random , and the network is simply a passive learner .
at this point , we need to draw attention to the distinction between a neural network 's architecture and its configuration .2 the architecture of a neural network refers to those parameters of the network that do not change during training ; in our case , these will be the network 's topology and transfer functions .
having trained on training set 5 ' , we can say that the network configuration implements a concept c that is consistent with training set 5m we will use c to denote both the concept c and the network .6 that implements it .
since we are interested in selecting examples that improve the generalization
this creates a background bias over the domain that is weighted by the input distribution 19 : the networks that have the least error on these background patterns will be the ones that are the most specific according to v. in order to allow the network to converge on the actual training examples in spite of the background examples , we must balance the influence of background examples against that of the training data .
