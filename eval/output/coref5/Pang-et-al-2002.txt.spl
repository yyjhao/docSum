this domain is experimentally convenient because there are large on-line collections of such reviews , and because reviewers often summarize their overall sentiment with a machine-extractable rating indicator , such as a number of stars ; hence , we did not need to hand-label the data for supervised learning or evaluation purposes .
these experiments also provide us with baselines for experimental comparison ; in particular , the third baseline of 69 % might actually be considered somewhat difficult to beat , since it was achieved by examination of the test data -LRB- although our examination was rather cursory ; we do 4later experiments using these words as features for machine learning methods did not yield better results .
because training maxent is expensive in the number of features , we limited consideration to -LRB- 1 -RRB- the 16165 unigrams appearing at least four times in our 1400 - document corpus -LRB- lower count cutoffs did not yield significantly different results -RRB- , and -LRB- 2 -RRB- the 16165 bigrams occurring most often in the same data -LRB- the selected bigrams all occurred at least seven times -RRB- .
this provides suggestive evidence that sentiment categorization is more difficult than topic classification , which corresponds to the intuitions of the text categorization expert mentioned above .10 nonetheless , we still wanted to investigate ways to improve our sentiment categorization results
