gildea -LRB- 2001 -RRB- simply added the out-of-domain treebank to his in-domain training data , and derived a very small benefit for his high accuracy , lexicalized parser , concluding that even a large amount of out-of-domain data is of little use for lexicalized parsing .
in their language modeling approach , in-domain counts are mixed with the out-of-domain model , so that , if the number of observations within the domain is small , the outof-domain model is relied upon , whereas if the number of observations in the domain is high , the model will move toward a maximum likelihood -LRB- ml -RRB- estimate on the in - domain data alone .
this may be because of the same issue as with the brown corpus , namely that the optimal ratio of in-domain to out-of-domain is not 1 and the smoothing parameters need to be tuned to the new domain ; or it may be because the new domain has a million words of training data , and hence has less use for out-of-domain data .
bacchiani and roark -LRB- 2003 -RRB- presented unsupervised map adaptation results for n-gram models , which use the same methods outlined above , but rather than using
in our case , we use the parsing model trained on out-of-domain data , and output a set of candidate parse trees for the strings in the in-domain corpus , with their normalized scores .
