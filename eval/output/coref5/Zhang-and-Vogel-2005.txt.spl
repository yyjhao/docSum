we use suffix array -LRB- manber 1990 -RRB- to index the training corpus and a novel fast algorithm to search all the substrings -LRB- phrases -RRB- of the testing sentences in the training data .
in the offline tm training approach , where one enumerates all the source phrases in the bilingual corpus and extracts their possible translations , it is clear that one better not to store translations for phrases longer than 3 words , otherwise the decoder is not able to load the phrase translation model during decoding .
we now modify the ibm1 alignment model by not summing the lexicon probabilities of all target words , but by restricting this summation in the following ways : for words inside the source phrase we sum only over the probabilities for words inside the target phrase candidate , and for words outside of the source phrase we sum only over the probabilities for the words outside the target phrase candidates ; the position alignment probability , which for the standard ibm1 alignment is 1 \/ i , where i is the number of words in the target sentence , is modified to 1 \/ l inside the source phrase and to 1 \/ -LRB- i-l -RRB- outside the source phrase .
to keep the phrasal translation model of a reasonable size , some
