first , we create clusters via complete-link clustering , measuring sentence similarity by the cosine metric using word bigrams as features -LRB- figure 1 shows example output -RRB- .4 then , given our knowledge that documents may sometimes discuss new and \/ or irrelevant content as well , we create an etcetera cluster by merging together all clusters containing fewer than sentences , on the assumption that such clusters consist of outlier sentences .
we demonstrate this point by utilizing content models to select appropriate sentence orderings : we simply use a content model trained on documents from the domain of interest , selecting the ordering among all the presented candidates that the content model assigns the highest probability to .
table 3 gives further details on the rank results for our content models , showing how the rank scores were distributed ; for instance , we see that on the earthquakes domain , the oso was one of the top five permutations in 95 % of the test documents .
content 2.67 72 % 0.81 earthquakes lapata -LRB- n \/ a -RRB- 24 % 0.48 bigram 485.16 4 % 0.27 content 3.05 48 % 0.64 clashes lapata -LRB- n \/ a -RRB- 27 % 0.41 bigram 635.15 12 % 0.25 content 15.38 38 % 0.45 drugs lapata -LRB- n \/ a -RRB- 27 % 0.49 bigram 712.03 11 % 0.24 content 0.05 96 % 0.98 finance lapata -LRB- n \/ a -RRB- 18 % 0.75 bigram 7.44 66 % 0.74 content 10.96 41 % 0.44 accidents lapata -LRB- n \/ a -RRB- 10 % 0.07 bigram 973.75 2 % 0.19 table 2 : ordering results
