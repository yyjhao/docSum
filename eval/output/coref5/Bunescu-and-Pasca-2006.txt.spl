the second step constructs the actual dictionary d as follows : the set of entries in d consists of all strings that may denote a named entity , i.e. if e e e is a named entity , then its title name e.title , its redirect names e.r , and its disambiguation names e.d are all added as entries in d. each entry string d e d is mapped to d .
the feature vector -LRB- d -LRB- q , ek -RRB- contains a dedicated feature 0eos for cosine similarity , and i v i x i c features 0w , e corresponding to combinations of words w from the wikipedia vocabulary v and categories c from the wikipedia taxonomy c : the weight vector w models the magnitude of each word-category correlation , and can be learned by training on the query dataset described at the beginning of section 4 .
it is straightforward to show that the dot product between two feature vectors -LRB- d -LRB- q , ek -RRB- and -LRB- d -LRB- q , e -RRB- is equal with the
as can be seen in the last two columns , the taxonomy kernel significantly outperforms the cosine similarity in the first three scenarios , confirming our intuition that correlations between words from the query context and categories from wikipedia taxonomy provide useful information for disambiguating named entities .
