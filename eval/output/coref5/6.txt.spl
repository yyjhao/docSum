the generalization problem is posed as follows : for a given concept class c , an unknown target t , an arbitrary error rate e , and confidence 6 , how many examples do we have to draw and classify from an arbitrary distribution -LRB- p in order to find a concept c e c consistent with the examples such that e -LRB- c , t , -LRB- p -RRB- 5 .
the backpropagation algorithm -LRB- rumelhart et al. , 1986 -RRB- is a supervised neural network learning technique , in that the network is presented with a training set of input \/ output pairs -LRB- x , t -LRB- x -RRB- -RRB- and learns to output t -LRB- x -RRB- when given input x .
having trained on training set 5 ' , we can say that the network configuration implements a concept c that is consistent with training set 5m we will use c to
this creates a background bias over the domain that is weighted by the input distribution 19 : the networks that have the least error on these background patterns will be the ones that are the most specific according to v. in order to allow the network to converge on the actual training examples in spite of the background examples , we must balance the influence of background examples against that of the training data .
