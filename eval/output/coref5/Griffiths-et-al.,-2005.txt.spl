in addition to producing clean syntactic and semantic classes and identifying function and content words , our composite model is competitive in quantitative tasks , such as part-of-speech tagging and document classification , with models specialized to detect only one kind of dependency .
first , we introduce the approach , considering the general question of how syntactic and semantic
for example , replacing the content words that the model identifies in the second sentence with content words appropriate to the topic of the present paper , we could write : the integrated architecture in this paper combines simple probabilistic syntax and topic-based semantics using generative models .
in addition to running the composite model with t = 200 and c = 20 , we examined two special cases : t = 200 , c = 2 , being a model where the only hmm classes are the start \/ end and semantic classes , and thus equivalent to latent dirichlet allocation -LRB- lda ; -LSB- 6 -RSB- -RRB- ; and t = 1 , c = 20 , being an hmm in which the semantic class distribution does not vary across documents , and simply has a different hyperparameter from the other classes .
each component divides words into finer groups according to a different criterion : the function words are divided into syntactic classes , and the content words are divided into semantic topics .
