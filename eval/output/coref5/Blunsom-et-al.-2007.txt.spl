for this reason , to our knowledge , all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures , such that spurious ambiguity is lessened or removed entirely -LRB- ittycheriah and roukos , 2007 ; watanabe et al. , 2007 -RRB- , or else ignore the problem and treat derivations as translations -LRB- liang et al. , 2006 ; tillmann and zhang , 2007 -RRB- .
the distribution is globally normalised by the partition function , za -LRB- f -RRB- , which sums out the numerator in -LRB- 1 -RRB- for every derivation -LRB- and therefore every translation -RRB- of f : given -LRB- 1 -RRB- , the conditional probability of a target translation given the source is the sum over all of its derivations : most prior work in smt , both generative and discriminative , has approximated the sum over derivations by choosing a single best derivation using a viterbi or beam search algorithm .
our model evaluation was motivated by the following questions : -LRB- 1 -RRB- the effect of maximizing translations rather than derivations in training and decoding ; -LRB- 2 -RRB- whether a regularised model performs better than a maximum likelihood model ; -LRB- 3 -RRB- how the performance of our model compares with a frequency count based hierarchical system ; and -LRB- 4 -RRB- how translation performance scales with the number of training examples .
in decoding we can search for the maximum probability
