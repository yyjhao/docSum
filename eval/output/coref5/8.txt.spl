we begin by reviewing the use of logistic regression in text classification , and the bayesian approach in particular -LRB- section 2 -RRB- , then discuss previous approaches to integrating domain knowledge in text classification -LRB- section 3 -RRB- .
chelba and acero -LSB- 5 -RSB- use out-of-task labeled examples in logistic regression training of a text capitalizer , and use the resulting map estimate as the mode vector of a bayesian prior for training with in-task examples .
another simple baseline is to create x copies of the prior knowledge text for a class and add these copies to the training data as additional positive examples -LRB- dk examples in table 1 -RRB- , as in some relevance feedback approaches .
our primary hypothesis was that using domain knowledge texts would greatly improve classifier effectiveness when few training examples are available , and not hurt
table 2 summarizes the types of domain knowledge used , and the number of domain knowledge texts used to compute significance values for the var \/ tfidf and mode \/ tfidf methods .
the resulting training sets had 2 to 139 positive examples for categories in the bio articles collection , 9 to 184 positive examples for categories in the modapte top 10 collection , and 0 to 22 positive examples for categories in the rcv1 a-b regions collection .
