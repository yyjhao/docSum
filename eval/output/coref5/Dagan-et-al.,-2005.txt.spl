as in other evaluation tasks our definition of textual entailment is operational , and corresponds to the judgment criteria given to the annotators who decide whether this relationship holds between a given pair of texts or not .
within each application setting the annotators selected both positive entailment examples -LRB- true -RRB- , where t is judged to entail h , as well as negative examples -LRB- false -RRB- , where entailment does not hold -LRB- a 50 % -50 % split -RRB- .
for this task the annotators used an available dataset annotated for the ie relations '' kill '' and '' birth place '' produced by uiuc -LRB- see acknowledgments -RRB- , as well as general news stories in which they identified manually '' typical '' ie relations .
the remaining examples were considered as the gold standard for evaluation , split to 567 examples in the development set and 800 in the test set , and evenly split to true \/ false examples .
judgments of the test examples were sorted by their confidence -LRB- in decreasing order -RRB- , calculating the following measure : the confidence-weighted score ranges between 0 -LRB- no correct judgments at all -RRB- and 1 -LRB- perfect classification -RRB- , and
sixteen groups submitted the results of their systems for the challenge data , while one additional group submitted the results of a manual analysis of the dataset -LRB- vanderwende et al. , see below -RRB- .
