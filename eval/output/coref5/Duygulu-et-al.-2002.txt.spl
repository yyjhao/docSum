by saying that words that can be reliably attached to image regions are easy to recognize and those that can not , are not ; and which objects are indistinguishable using our features ?
this can occur because one word is a modifier for example , in our data set , polar reliably predicts bear or because of some relation between the
evaluation method : each image in the test set is automatically annotated , by taking every region larger than the threshold , quantizing the region to a blob , and using the lexicon to determine the most likely word given that blob ; if the probability of the word given the blob is greater than the relevant threshold , then the image is annotated with that word .
the effect of retraining : since we can predict only 80 words , we can reduce our vocabulary only to those words , and run em algorithm again .
table 1 and table 2 shows that , with the increasing null threshold values , the number of words decreases but we have more reliable words .
table 3 shows that we have some very nice clusters which have strong semantic or visual relations like kit-horses-mare-foals , leaf-flowers-plants-vegetables or pool-athlete-vines-swimmers and the results are better when we cluster the words -LRB- compare with table 1 -RRB- .
