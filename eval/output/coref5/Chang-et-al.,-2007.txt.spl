given a small amount of labeled data and a large unlabeled pool , our framework
we note that in the presence of constraints , the inference procedure -LRB- for finding the output y that maximizes the cost function -RRB- is usually done with search techniques -LRB- rather than viterbi decoding , see -LRB- toutanova et al. , 2005 ; roth and yih , 2005 -RRB- for a discussion -RRB- , we chose beamsearch decoding .
the fact that the constraints are used in the inference procedure -LRB- in particular , for generating new training examples -RRB- allows us to use a learning algorithm that ignores the constraints , which is a lot more efficient -LRB- although algorithms that do take the constraints into account can be used too -RRB- .
figure 3 compares two protocols on the advertisements domain : h &amp; w + i , where we first run the h &amp; w protocol and then apply the constraints during testing stage , and h &amp; w &amp; c + i , which uses constraints to guide the model during learning and uses it also in testing .
the results show that constraints improve not only the performance of the final inference stage but also propagate useful information during the semi - supervised learning process and that training with the constraints is especially significant when the number of labeled training data is small .
