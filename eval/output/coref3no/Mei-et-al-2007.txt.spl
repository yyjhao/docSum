in figure 1 , given a query word representing a users ad hoc information need -LRB- e.g. , a product -RRB- , the system extracts the latent facets -LRB- subtopics -RRB- in the search results , and associates
in the previous work -LSB- 15 , 17 -RSB- , the words in a blog article are classified into two categories : -LRB- 1 -RRB- common english words -LRB- e.g. , the , a , of -RRB- and -LRB- 2 -RRB- words related to a topical theme -LRB- e.g. , nano , price , mini in the documents about ipod -RRB- .
specifically , for the words related to a topic , we further categorize them into three sub-categories : -LRB- 1 -RRB- words about the topic with neutral opinions -LRB- e.g. , nano , price -RRB- ; -LRB- 2 -RRB- words representing the positive opinions of the topic -LRB- e.g. , awesome , love -RRB- ; and -LRB- 3 -RRB- words representing the negative opinions about the topic -LRB- e.g. , hate , bad -RRB- .
we define the following two conjugate dirichlet priors for the sentiment model ^ p and ^ n , respectively : dir -LRB- -LCB- 1 + pp -LRB- wi ^ p -RRB- -RCB- wev -RRB- and dir -LRB- -LCB- 1 + np -LRB- wi ~ ^ n -RRB- -RCB- wev -RRB- , where the parameters p and n indicate how strong our confidence is on the sentiment model prior .
with this model , we could effectively -LRB- 1 -RRB- learn general sentiment models ; -LRB- 2 -RRB- extract topic models orthogonal to sentiments , which can represent the neutral content of a subtopic ; and -LRB- 3 -RRB- extract topic life cycles and the associated sentiment dynamics .
