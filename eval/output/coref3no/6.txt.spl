if we draw at random over the whole domain , then the probability that an individual sample will reduce our error is a , as defined above , which decreases to zero as we draw more and more examples .
rather than assuming that the drawing of a classified example is an atomic operation -LRB- see valiant , 1984 ; blumer et al. , 1988 -RRB- , we may divide the operation into two steps : first , drawing an unclassified example from the distribution , and second , querying the classification of that point .
the backpropagation algorithm -LRB- rumelhart
in the following two subsections , we first describe how one may learn a '' most specific '' or '' most general '' concept associated with a network and then describe how these two networks may be used to selectively sample the -LRB- r \* -LRB- r -RRB- defined by regions where they disagree .
this creates a background bias over the domain that is weighted by the input distribution 19 : the networks that have the least error on these background patterns will be the ones that are the most specific according to v. in order to allow the network to converge on the actual training examples in spite of the background examples , we must balance the influence of background examples against that of the training data .
