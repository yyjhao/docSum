again , we assume independence from the history , so that the contribution of p -LRB- ri -RRB- for any ordering of a fixed set of sentences is constant and we omit it : estimating p -LRB- ei i ri , ~ rh -LRB- i _ 1 -RRB- , j -RRB- proves to be difficult , since the contexts are very sparse .
to overcome this difficulty we simply normalize by force3 : the individual probabilities p -LRB- r + ej i r , rh -LRB- i _ 1 -RRB- , j -RRB- are calculated by counting situations in the training documents in which a known noun has history i h -LRB- i _ 1 -RRB- , j and fills slot r in the next sentence , versus situations where the slot r exists but is filled by some other noun .
our variant of it , unlike
our experiments use the popular airplane corpus , a collection of documents describing airplane crashes taken from the database of the national transportation safety board , used in -LRB- barzilay and lee , 2004 ; barzilay and lapata , 2005 ; soricut and marcu , 2006 -RRB- .
in the sentence ordering task , -LRB- lapata , 2003 ; barzilay and lee , 2004 ; barzilay and lapata , 2005 ; soricut and marcu , 2006 -RRB- , we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model .
