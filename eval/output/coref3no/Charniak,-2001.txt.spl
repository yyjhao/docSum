for example , defense secretary george w. smith would be a name and we would analyze it into the components defense secretary -LRB- a descriptor -RRB- , george -LRB- a first name -RRB- , w. -LRB- a middle name , we do not distinguish between initials and true names -RRB- , and smith -LRB- a last name -RRB- .
we assume that peoples names have six -LRB- optional -RRB- components as exemplified in the following somewhat contrived example : our models make the following assumptions about personal names : all words of label l -LRB- the label number -RRB- must occur before all words of label l + 1 , with the exception of descriptors , a maximum of one word may appear for each label , every name must include either a first name or a last name , in a loose sense , honorifics and closes are closed classes , even if we do not know which words are in the classes .
for the name model we straightforwardly used equation 2 to determine the most probable label sequence l ~ for
lastly , there are situations where we imagine that if the program had more data -LRB- or if the learning mechanisms were somehow better -RRB- it would get the example right .
the two methods differ in that the second uses multiple , possibly coreferent , occurrences of names to constrain the problem .
