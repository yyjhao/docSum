thus , using superscripts to denote components , the vth word in the vocabulary is represented by
given the parameters a and r , the joint distribution of a topic mixture 0 , a set of n topics z , and a set of n words w is given by : we refer to the latent multinomial variables in the lda model as topics , so as to exploit text-oriented intuitions , but we make no epistemological claims regarding these latent variables beyond their utility in representing probability distributions on sets of words .
in this section we compare lda to simpler latent variable models for textthe unigram model , a mixture of unigrams , and the plsi model .
however , variational inference provides us with a tractable lower bound on the log likelihood , a bound which we can maximize with respect to a and r. we can thus find approximate empirical bayes estimates for the lda model via an alternating variational em procedure that maximizes a lower bound with respect to the variational parameters y and 0 , and then , for fixed values of the variational parameters , maximizes the lower bound with respect to the model parameters a and r. we provide a detailed derivation of the variational em algorithm for lda in appendix a. 4 .
