we assume that we are given two sets of training data , d -LRB- -RRB- and d -LRB- i -RRB- , the out-of-domain and in - domain data sets , respectively .
under this modified notation , we may rewrite eq -LRB- 2 -RRB- as : the parameters a can be estimated using any convex optimization technique ; in practice , limited memory bfgs -LRB- nash
this bound is attractive for other reasons : -LRB- 1 -RRB- it is tangent to the logarithm ; -LRB- 2 -RRB- it is tight ; -LRB- 3 -RRB- it makes contact at the current operating point -LRB- according to the maximization at the previous time step -RRB- ; -LRB- 4 -RRB- it is a simply linear function ; and -LRB- 5 -RRB- in the terminology of the calculus of variations , it is the variational dual to the logarithm ; see -LRB- smith , 1998 -RRB- .
applying jensens inequality to the first term in eq -LRB- 9 -RRB- and the variational dual to the second term , we obtain that the change of log-likelihood in moving from model parameters ot-1 at time t 1 to ot at time t -LRB- which we shall denote qt -RRB- is bounded by al &gt; qt , where qt is defined by eq -LRB- 10 -RRB- , where h = -RSB- e -LCB- z i x ; o -RCB- when z = 1 and 1 -RSB- e -LCB- z i x ; o -RCB- when z = 0 , with expectations taken with respect to the parameters from the previous iteration .
