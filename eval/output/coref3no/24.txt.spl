a growing number of statistical classification methods and machine learning techniques have been applied to text categorization in recent years , including multivariate regression models -LSB- 8 , 27 -RSB- , nearest neighbor classification -LSB- 4 , 23 -RSB- , bayes probabilistic approaches -LSB- 20 , 13 -RSB- , decision trees -LSB- 13 -RSB- , neural networks -LSB- 21 -RSB- , symbolic rule learning -LSB- 1 , 16 , 3 -RSB- and inductive learning algorithms -LSB- 3 , 12 -RSB- .
these criteria are : document frequency -LRB- df -RRB- , information gain -LRB- ig -RRB- , mutual information -LRB- mi -RRB- , a xz statistic -LRB- chi -RRB- , and term strength -LRB- ts -RRB- .
to measure the goodness of a term in a global feature selection , we combine the category - specific scores of a term into two alternate ways : the mi computation has a time complexity of o -LRB- vm -RRB- , similar to
4.3 primary results figure 1 displays the performance curves for knn on reuters -LRB- 9,610 training documents , and 3,662 test documents -RRB- after term selection using ig , df , ts , mi and chi thresholding , respectively .
table 1 compares the five criteria from several angles : from table 1 , the methods with an '' excellent '' performance share the same bias , i.e. , scoring in favor of common terms over rare terms .
we discovered that the df , ig and chi scores of a term are strongly correlated , revealing a previously unknown fact about the importance of common terms in text categorization .
