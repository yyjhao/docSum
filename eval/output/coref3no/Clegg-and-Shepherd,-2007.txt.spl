for each parser , we calculated two scores , constituent effectiveness -LRB- fconst -RRB- and dependency effectiveness -LRB- fdep -RRB- against the original constituent trees in the treebank , and their dependency graph equivalents , respectively -LRB- see tables 1 and 2 -RRB- .
for example , if we were comparing the incorrect parse in figure 8 to the sentence in figure 7 , our gold standard would consist of all the dependencies from figure 7 that go to or from
in order to detect any latent parsing problems that might hinder this process , we chose one of the most common biological predicate verbs in the corpus -LRB- ' induce ' in any of its forms -RRB- and divided the dependency types that can hold between it and its -LRB- non-prepositional -RRB- arguments into two sets : those which one would expect to find linking it to its agent , and those which one would expect to find linking it to its target .
they report fdep scores of 88.5 and 92.0 for identifying the subjects and objects of verbs respectively , although it is not clear whether or not these relation types are defined as broadly as the categories we used above in the study of the verb ' induce ' , where the charniak-lease parser scored 98.0 and the bikel parser scored 97.0 , averaged across both agent and target relations .
