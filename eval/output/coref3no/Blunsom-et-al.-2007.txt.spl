our model evaluation was motivated by the following questions : -LRB- 1 -RRB- the effect of maximizing translations rather than derivations in training and decoding ; -LRB- 2 -RRB- whether a regularised model performs better than a maximum likelihood model ; -LRB- 3 -RRB- how the performance of our model compares with a frequency count based hierarchical system ; and -LRB- 4 -RRB- how translation performance scales with the number of training examples .
in decoding we can search for the maximum probability derivation , which is the standard practice in smt , or for the maximum probability translation which is what we actually want from our model , i.e. the best translation .
to our knowledge no systems directly address problem 1 , instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list -LRB- liang et al. , 2006 ; watanabe et al. , 2007 -RRB- , or else making local independence assumptions which side-step the issue -LRB- ittycheriah and roukos , 2007 ; tillmann and zhang , 2007 ; wellington et al. , 2006 -RRB- .
we present our model in section 3 , including our means of training and decoding .
to compare our model directly with these systems we would need to incorporate additional features and a language model , work which we have left for a later date .
the relative scores confirm that our model , with
