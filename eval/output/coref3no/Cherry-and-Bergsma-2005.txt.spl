ems role is to induce a probability distribution over candidates to maximize the likelihood of the -LRB- p , k -RRB- pairs observed in our training set : to improve our ability to generalize to future cases , we use a naive bayes assumption to state that the choices of pronoun and context are conditionally independent , given an antecedent .
that is , once we select the word the pronoun represents , the pronoun and its context are no longer coupled : we can split each candidate c into its lexical component l and its jump value j .
if we assume that l and j are independent , and that p and k each depend only on the l component of c , we can combine equations 3 and 4 to get our final formulation for the joint probability distribution : the jump term j , though important when resolving pronouns , is not likely to be correlated with any lexical choices in the training set .
even though we have justified equation 5 with reasonable independence assumptions , our four models may not be combined optimally for our pronoun resolution task , as the models are only approximations of the true distributions they are intended to represent .
following the approach in -LRB- och and ney , 2002 -RRB- , we
