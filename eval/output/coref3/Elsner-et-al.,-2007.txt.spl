the probability of a sentence is now : we make one further simplification before beginning to approximate : we first generate the set of syntactic slots ri which we intend to fill with known entities , and then decide which entities from the known set to select .
to overcome this difficulty we simply normalize by force3 : the individual probabilities p -LRB- r + ej i r , rh -LRB- i _ 1 -RRB- , j -RRB- are calculated by counting situations in the training documents in which a known noun has history i h -LRB- i _ 1 -RRB- , j and fills slot r in the next
the emission model of each state is an instance of the relaxed entity grid model as described above , but in addition to conditioning on the role and history , we condition also on the state and on the particular set of lexical items lex -LRB- ki -RRB- which may be selected to fill the role : p -LRB- r + _ ej i r , ~ rh -LRB- i-1 -RRB- , j , qi , lex -LRB- ki -RRB- -RRB- .
in the sentence ordering task , -LRB- lapata , 2003 ; barzilay and lee , 2004 ; barzilay and lapata , 2005 ; soricut and marcu , 2006 -RRB- , we view a document as an unordered bag of sentences and try to find the ordering of the sentences which maximizes coherence according to our model .
