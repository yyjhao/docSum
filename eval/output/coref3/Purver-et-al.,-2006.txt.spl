however , while hmm approaches allow a segmentation of the data by topic , they do not allow adaptively combining different
for each set of annotations , we therefore performed two sets of segmentations : one in which the threshold was set for each meeting to give the known gold - standard number of segments , and one in which the threshold was set on a separate development set to give the overall corpus-wide average number of segments , and held constant for all test meetings .2 this also allows us to compare our results with those of -LRB- galley et al. , 2003 -RRB- , who apply a similar threshold to their lexical cohesion function and give corresponding results produced with known \/ unknown numbers of segments .
of these lists , 40 contained the most indicative words for each of the 10 topics from different models : the topic segmentation model ; a topic model that had the same number of segments but with fixed evenly spread segmentation boundaries ; an equivalent with randomly placed segmentation boundaries ; and the hmm .
following previous work on probabilistic topic models -LRB- hofmann , 1999 ; blei et al. , 2003 ; griffiths and steyvers , 2004 -RRB- , we model each utterance as being generated from a particular distribution over topics , where each topic is a probability distribution over words .
