sch olkopf -LSB- 14 -RSB- showed that the prior knowledge can be incorporated with the appropriate kernel function , and fung -LSB- 4 -RSB- showed prior knowledge in the form of multiple polyhedral sets can be
given a set of vectors -LRB- x1 , ... , xn -RRB- , along with their corresponding labels -LRB- y1 , ... , yn -RRB- where yi g -LCB- + 1 , 11 , the svm classifier defines a hyperplane -LRB- w , b -RRB- in kernel mapped feature space that separates the training data by a maximal margin .
given the true training dataset and pseudo training dataset , we now have two possibly conflicting goals in minimizing the empirical risk when constructing a predictor : -LRB- 1 -RRB- fit the true training dataset , and -LRB- 2 -RRB- fit the pseudo training dataset and thus fit the prior knowledge .
let the first m training examples be the labeled examples , and the rest be the pseudo examples , the objective function of primal problem is given : here the functionality of the parameter c is the same as the standard svm to control the balance between the model complexity and training error .
the performance of wmsvm with the prior knowledge on reuters is comparable to that of transductive svm -LSB- 8 -RSB- , but the training time is much less as only one iteration of svm training is needed .
