the most straightforward way of implementing this idea is in the form of a backoff scheme : if the n-gram count for an item in the corpus falls below a threshold 0 , the web is used to estimate
they presented a simple method for retrieving bigram counts from the web by querying a search engine and demonstrated that web counts -LRB- a -RRB- correlate with frequencies obtained from a carefully edited , balanced corpus such as the 100m words british national corpus -LRB- bnc -RRB- , -LRB- b -RRB- correlate with frequencies recreated using smoothing methods in the case of unseen bigrams , -LRB- c -RRB- reliably predict human plausibility judgments , and -LRB- d -RRB- yield state-of-the-art performance on pseudo-disambiguation tasks .
these models then form the basis for the significance tests reported in table v. -RRB- as explained in section 2.5 , we also test two weakly supervised models that combine web counts and corpus counts using backoff and interpolation .
note that for certain tasks , the performance of a web baseline model might actually be sufficient , so that the effort of constructing a sophisticated supervised model and annotating the necessary training data can be avoided : recall that for three tasks , our web-based models outperformed the best model in the literature -LRB- for mt candidate selection , article generation , and compound interpretation , see table xx -RRB- .
