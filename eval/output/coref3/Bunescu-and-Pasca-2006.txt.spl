for a given wikipedia entity e g e , let e. 0 be the set of categories to which e belongs -LRB- i.e. e s immediate categories and all their ancestors in the wikipedia taxonomy -RRB- .
the second step constructs the actual dictionary d as follows : the set of entries in d consists of all strings that may denote a named entity , i.e. if e e e is a named entity , then its title name e.title , its redirect names e.r , and its disambiguation names e.d are all added as entries in d. each entry string d e d is mapped to d .
the feature vector -LRB- d -LRB- q , ek -RRB- contains a dedicated feature 0eos for cosine similarity , and i v i x i c features 0w , e corresponding to combinations of words w from the wikipedia vocabulary v and categories c from the wikipedia taxonomy c : the weight vector w models the magnitude of each word-category correlation , and can be learned by training on the query dataset described at the beginning of section 4 .
as can be seen in the last two columns , the taxonomy kernel significantly outperforms the cosine similarity in the first three scenarios , confirming our intuition that correlations between words from the query context and categories from wikipedia taxonomy
