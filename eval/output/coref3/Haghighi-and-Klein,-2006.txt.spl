the overall approach of grenager et al. -LRB- 2005 -RRB- typifies the process involved in fully unsupervised learning on new domain : they first alter the structure of their hmm
for each document x = -LSB- xi -RSB- , we would like to predict a sequence of labels y = -LSB- yi -RSB- , where xi e x and yi e y. we construct a generative model , p -LRB- x , y10 -RRB- , where 0 are the model s parameters , and choose parameters to maximize the log-likelihood of our observed data d : markov random fields .
we automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often .
each prototype word is also its own prototype -LRB- since a word has maximum similarity to itself -RRB- , so when we lock the prototype to a label , we are also pushing all the words distributionally similar to that prototype towards that label .
adding distributional similarity fea tures to our model -LRB- proto + sim -RRB- improves accuracy substantially , yielding 71.5 % , a 38.4 % error reduction over base .6 another feature of this domain that grenager et al. -LRB- 2005 -RRB- take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one .
