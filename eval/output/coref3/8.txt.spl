numerous studies show that effective text classifiers can be produced by supervised learning methods , including support vector machines -LRB- svms -RRB- -LSB- 11 , 14 , 33 -RSB- , regularized logistic regression -LSB- 9 , 33 -RSB- , and other approaches -LSB- 14 , 27 , 33 -RSB- .
several recent papers have modified learning approaches naive bayes -LSB- 13 , 16 -RSB- , logistic regression -LRB- fit with a boosting - style algorithm -RRB- -LSB- 25 -RSB- , and svms -LSB- 31 -RSB- to use domain knowledge in text categorization .
chelba and acero -LSB- 5 -RSB- use out-of-task labeled examples in logistic regression training of a text capitalizer , and use the resulting map estimate as the mode vector of a bayesian prior for training with in-task examples .
some papers explore several values for the prior variance -LSB- 15 , 33 -RSB- , others use a single
we note that our macroaveraged f1 for svms on modapte top 10 -LRB- 86.55 -RRB- is similar to that found by wu &amp; srihari -LRB- approximately 83.5 on a non-random sample of 1,024 training examples , from the graph in figure 3 -LSB- 31 -RSB- -RRB- and joachims -LRB- 82.5 with all 9,603 training examples , computed from his figure 2 -LSB- 11 -RSB- -RRB- .
the resulting training sets had 2 to 139 positive examples for categories in the bio articles collection , 9 to 184 positive examples for categories in the modapte top 10 collection , and 0 to 22 positive examples for categories in the rcv1 a-b regions collection .
