thus , our mt evaluation system requires two ingredients : a numerical translation closeness metric , a corpus of good quality human reference translations .
but first observe that candidate 1 shares '' it is a guide to action '' with reference 1 , '' which '' with reference 2 , '' ensures that the military '' with reference 1 , '' always '' with references 2 and 3 , '' commands '' with reference 1 , and finally '' of the party '' with reference 2 -LRB- all ignoring capitalization -RRB- .
it is
to verify that modified n-gram precision distinguishes between very good translations and bad translations , we computed the modified precision numbers on the output of a -LRB- good -RRB- human translator and a standard -LRB- poor -RRB- machine translation system using 4 reference translations for each of 127 source sentences .
however , as can be seen in figure 2 , the modified n-gram precision decays roughly exponentially with n : the modified unigram precision is much larger than the modified bigram precision which in turn is much bigger than the modified trigram precision .
the figure also highlights the relatively large gap between mt systems and human translators .8 in addition , we surmise that the bilingual group was very forgiving in judging h1 relative to h2 because the monolingual group found a rather large difference in the fluency of their translations .
