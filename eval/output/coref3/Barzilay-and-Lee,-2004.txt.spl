first , we create
we demonstrate this point by utilizing content models to select appropriate sentence orderings : we simply use a content model trained on documents from the domain of interest , selecting the ordering among all the presented candidates that the content model assigns the highest probability to .
content 2.67 72 % 0.81 earthquakes lapata -LRB- n \/ a -RRB- 24 % 0.48 bigram 485.16 4 % 0.27 content 3.05 48 % 0.64 clashes lapata -LRB- n \/ a -RRB- 27 % 0.41 bigram 635.15 12 % 0.25 content 15.38 38 % 0.45 drugs lapata -LRB- n \/ a -RRB- 27 % 0.49 bigram 712.03 11 % 0.24 content 0.05 96 % 0.98 finance lapata -LRB- n \/ a -RRB- 18 % 0.75 bigram 7.44 66 % 0.74 content 10.96 41 % 0.44 accidents lapata -LRB- n \/ a -RRB- 10 % 0.07 bigram 973.75 2 % 0.19 table 2 : ordering results -LRB- averages over the test cases -RRB- .
domain rank range -LSB- 0-4 -RSB- -LSB- 5-10 -RSB- earthquakes 95 % 1 % 4 % clashes 75 % 18 % 7 % drugs 47 % 8 % 45 % finance 100 % 0 % 0 % accidents 52 % 7 % 41 % table 3 : percentage of cases for which the content model assigned to the oso a rank within a given range .
while our experiments are limited to only one domain , the correlation in results is encouraging : optimizing parameters on one task promises to yield model size 10 20 40 60 64 80 ordering 11 % 28 % 52 % 50 % 72 % 57 % summarization 54 % 70 % 79 % 79 % 88 % 83 % table 5 : content-model performance on earthquakes as a function of model size .
