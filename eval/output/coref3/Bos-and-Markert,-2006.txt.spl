we will introduce the shallow semantic analysis -LRB- section 2 -RRB- and the deep semantic analysis -LRB- section 3 -RRB- , present the results of our two runs -LRB- section 4 -RRB- , and discuss them -LRB- section 5 -RRB- .
in addition , we use four other shallow features : textlength measuring the length of t in words , hyplength measuring the length of the hypothesis and proplength measuring the difference between textlength and h len th as textlength the last shallow feature task simply uses the task variable -LRB- one of sum , qa , ie , ir -RRB- as our results in rte-1 -LRB- bos and markert , 2005 -RRB- showed that the different tasks can need different inference methods .
we perform model building using two different model builders : we use paradox to find the size of the domain , and then use mace to construct a minimal model giving that domain size .
else , if there is no proof for 3 or 4 , but there is a proof for 2 , then again it is very likely that t entails h. finally , if the model size difference between the models generated for 5 and 6 is very small then it is likely that t entails h. background knowledge .
we generate background knowledge -LRB- bk -RRB- using two kinds of sources : hyponymy relations from wordnet , and a
