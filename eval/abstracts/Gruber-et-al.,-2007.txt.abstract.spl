algorithms such as latent dirichlet allocation ( lda ) have achieved significant progress in modeling word document relationships .
these algorithms assume each word in the document was generated by a hidden topic and explicitly model the word distribution of each topic as well as the prior distribution over topics in the document .
given these parameters , the topics of all words in the same document are assumed to be independent .
in this paper , we propose modeling the topics of words in the document as a markov chain .
specifically , we assume that all words in the same sentence have the same topic , and successive sentences are more likely to have the same topics .
since the topics are hidden , this leads to using the well-known tools of hidden markov models for learning and inference .
we show that incorporating this dependency allows us to learn better topics and to disambiguate words that can belong to different topics .
quantitatively , we show that we obtain better perplexity in modeling documents with only a modest increase in learning and inference complexity .
